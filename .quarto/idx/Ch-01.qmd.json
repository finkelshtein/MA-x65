{"title":"Loss distributions","markdown":{"yaml":{"format":"html"},"headingText":"Loss distributions","containsRefs":false,"markdown":"\n\n```{ojs}\n//| output: false\nmathfn = require('https://bundle.run/mathfn@1.1.0')\n\n```\n\n```{r}\n#| echo: false\nsec = 1\ncur = 0\n```\n\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-note icon=false}\n\n## `{r} sec`.`{r} cur` Probability distribution\n\n- Let $X:\\Omega\\to\\mathbb{R}$ be a continuous random variable.\n\n- The **cummulative distribution function (CDF)** of $X$ is a **non-decreasing** function\n$$\nF_X(x)=\\mathbb{P}(X\\leq x)\\in[0,1].\n$$ {#eq-1.1}\n\n- The **probability density function (PDF)** (a.k.a. **probability density**) of $X$ is\n$$\nf_X(x)=F'_X(x)\\geq0.\n$$ {#eq-1.2}\n\n- Relation:\n$$\n\\mathbb{P}(a\\leq X \\leq b) = \\int_a^b f_X(x)\\,dx=F_X(b)-F_X(a).\n$$ {#eq-1.3}\n\n- For a $k\\in\\mathbb{N}$, the **moment of order $k$** of $X$ is\n$$\nm_k=m_{k,X}=\\mathbb{E}(X^k)=\\int_\\mathbb{R} x^k \\, f_X(x)\\,dx.\n$$ {#eq-kthmoment}\n\n- The **moment generator function (MGM)** is\n$$\nM_X(t)=\\mathbb{E}(e^{tX})=\\int_\\mathbb{R} e^{tx} \\, f_X(x)\\,dx\\geq0.\n$$ {#eq-1.5}\n\n- Relation:\n$$\nM_X(t)=1+\\sum_{k=1}^\\infty\\frac{m_k}{k!}t^k.\n$$ {#eq-1.6}\n\n:::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-tip icon=false}\n\n## `{r} sec`.`{r} cur` Remark\n\n- Recall that functions $\\dfrac{1}{1+|x|^\\beta}$ and $\\dfrac{1}{(1+|x|)^\\beta}$ are integrable on $\\mathbb{R}$ iff $\\beta>1$.\n\n- Note that if $f_X(x)$ is continuous on $\\mathbb{R}$, it's integrable on arbitrary large interval $[-r,r]$, $r>0$. Therefore, if there exists $A>$ and $r>0$ such that\n$$\n|f_X(x)|\\leq \\frac{A}{1+|x|^\\beta}, \\qquad |x|>r,\n$$ {#eq-sufcondint}\nand $\\beta>1$, then indeed, $\\int\\limits_{\\mathbb{R}}|f_X(x)|\\,dx<\\infty$.\n\n- As a result, if\n$\\beta>k+1$ in ([-@eq-sufcondint]), then the $k$-th moment $m_k$ in ([-@eq-kthmoment]) is well defined and finite.\n\n- Examples of such probability densities $f_X(x)$ may be, apart from the obvious $\\dfrac{1}{1+|x|^\\beta}$ and $\\dfrac{1}{(1+|x|)^\\beta}$ with $\\beta>k+1$, also the considered below Weibull function\n$$\nf_X(x)=e^{-a |x|^b}\n$$ {#eq-Weibull}\nwith **any** positive $a,b>0$.\n\n- Moreover, if in ([-@eq-Weibull])\n$b>1$, then\n$0< M_X(t)<\\infty$.\n\n:::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-note icon=false}\n\n## `{r} sec`.`{r} cur` The exponential distribution\n\nA random variable $X:\\Omega\\to\\mathbb{R}$ has the **exponential distribution** with parameter $\\lambda>0$ if its CDF is\n$$\nF(x)=1-e^{-\\lambda x}, \\quad x\\geq0.\n$$ {#eq-1.9}\n\nHere and below, when we write a restriction on $x$, we mean that function is $0$ otherwise, i.e. here $F(x)=0$ for $x<0$.\n\nThen the PDF is\n$$\nf(x)=\\lambda e^{-\\lambda x}, \\quad x\\geq0.\n$$ {#eq-1.10}\n\nNext,\n$$\n\\mathbb{E}(X)=\\frac1{\\lambda},\n$$ {#eq-1.11} \n$$\n\\mathrm{Var}(X)=\\frac1{\\lambda^2}= \\bigl( \\mathbb{E}(X) \\bigr)^2,\n$$ {#eq-1.12}\n$$\n\\mathbb{E}(X^2)=\\frac2{\\lambda^2}=2\\bigl( \\mathbb{E}(X) \\bigr)^2.\n$$ {#eq-1.13}\nThe MGM if defined for $t<\\lambda$ only:\n$$\nM(t)=\\biggl(1-\\frac{t}{\\lambda}\\biggr)^{-1}=\\frac{\\lambda}{\\lambda-t}.\n$$\nThe notation is\n$$\nX\\sim Exp(\\lambda).\n$$ {#eq-1.14}\n\n:::\n\n::: {#fig-Exp}\n\n```{ojs}\n\nviewof l = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\lambda:` })\n\nviewof xmexp = Inputs.range([1, 100], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nymaxexp = d3.max(d3.range(0, xmexp, 0.01).map(t => l*Math.exp(-l * t)))\n\nPlot.plot({\n  x: { domain: [-0.1, xmexp] },\n  y: { domain: [-0.1*ymaxexp, ymaxexp*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    Plot.line(\n      d3\n        .range(0, xmexp, 0.1)\n        .map(t => [\n          t,\n          l*Math.exp(-l * t)\n        ]),{\n        strokeWidth: 3,\n        stroke: \"steelblue\"\n      }\n    ),\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: 0 })\n  ]\n})\n```\n\nGraph of $f(x)=\\lambda e^{-\\lambda x}$\n:::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-note icon=false}\n\n## `{r} sec`.`{r} cur` The gamma distribution\n\nA random variable $X:\\Omega\\to\\mathbb{R}$ has the **gamma distribution** with parameters $\\alpha>0$ and $\\lambda>0$ if its PDF is\n$$\nf(x)=\\frac{\\lambda^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1} e^{-\\lambda x}, \\quad x\\geq0.\n$$ {#eq-1.15}\nHere\n$$\n\\Gamma(\\alpha):=\\int_0^\\infty t^{\\alpha-1}e^{-t}\\,dt, \\quad \\alpha>0\n$$ {#eq-1.16}\nis the gamma function.\n\nThe following formulas hold:\n$$\n\\mathbb{E}(X)=\\frac{\\alpha}{\\lambda},\n$$ {#eq-1.17}\n$$\n\\mathrm{Var}(X)=\\frac{\\alpha}{\\lambda^2},\n$$ {#eq-1.18}\n$$\nM_X(t)=\\biggl(1-\\frac{t}{\\lambda}\\biggr)^{-\\alpha}=\\frac{\\lambda^\\alpha}{(\\lambda-t)^\\alpha}, \\quad t<\\lambda.\n$$ {#eq-1.19}\nThe notation is\n$$\nX\\sim \\Gamma(\\alpha,\\lambda).\n$$\n\n:::\n\n::: {#fig-gamma}\n\n```{ojs}\nviewof a = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\alpha:` })\nviewof l2 = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\lambda:` })\nviewof xmgamma = Inputs.range([1, 100], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nfunction gamma(t,alpha,lambda){\n  return ((lambda**alpha)/mathfn.gamma(alpha))*(t**(alpha-1))*Math.exp(-lambda * t);\n}\n\n\nymaxgamma = d3.max(d3.range(0.1, xmgamma, 0.001).map(t => gamma(t,a,l2)))\n\n\nPlot.plot({\n  x: { domain: [0.1, xmgamma] },\n  y: { domain: [-0.1*ymaxgamma, ymaxgamma*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    Plot.line(\n      d3\n        .range(0,xmgamma, 0.01)\n        .map(t => [t, gamma(t,a,l2)]),{\n        strokeWidth: 3,\n        stroke: \"steelblue\"\n      }\n    ),\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: 0 })\n  ]\n})\n```\n\nGraph of $f(x)=\\dfrac{\\lambda^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1} e^{-\\lambda x}$\n\n:::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-note icon=false}\n\n## `{r} sec`.`{r} cur` The normal distribution\n\nA random variable $X:\\Omega\\to\\mathbb{R}$ has the **normal distribution** with the mean $\\mu$ and the variance $\\sigma^2$ if\n$$\nf_X(x)=\\frac1{\\sigma\\sqrt{2\\pi}}e^{-\\frac1{2\\sigma^2}(x-\\mu)^2}. \n$$ {#eq-1.20}\nIndeed,\n$$\n\\mathbb{E}(X)=\\mu, \\quad \\mathrm{Var}(X)=\\sigma^2.\n$$ {#eq-1.21}\nThe MGF is defined now everywhere:\n$$\nM_X(t)=e^{\\mu t +\\frac12 \\sigma^2 t^2}, \\quad t\\in\\mathbb{R}.\n$$ {#eq-1.22}\nNotation\n$$\nX\\sim N(\\mu,\\sigma^2).\n$$\n:::\n\n::: {#fig-normal}\n\n```{ojs}\nviewof mu = Inputs.range([-0.7*xmgauss, 0.7*xmgauss], { value: 0, step: 0.001, label: tex`\\mu:` })\nviewof sigma = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\sigma:` })\nviewof xmgauss = Inputs.range([1, 100], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nfunction gauss(t,mu,sigma){\n  return Math.exp(-(1/(2*sigma**2))*(t-mu)**2)/(sigma*Math.sqrt(2*Math.PI));\n}\n\nymaxgauss = d3.max(d3.range(-xmgauss, xmgauss, 0.01).map(t => gauss(t,mu,sigma)))\n\nPlot.plot({\n  x: { domain: [-xmgauss, xmgauss] },\n  y: { domain: [-ymaxgauss*0.1, ymaxgauss*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    Plot.line(\n      d3\n        .range(-xmgauss, xmgauss, 0.01)\n        .map(t => [t, gauss(t,mu,sigma)]),{\n        strokeWidth: 3,\n        stroke: \"steelblue\"\n      }\n    ),\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: 0 })\n  ]\n})\n```\n\nGraph of $f_X(x)=\\dfrac1{\\sigma\\sqrt{2\\pi}}e^{-\\frac1{2\\sigma^2}(x-\\mu)^2}$\n\n:::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-note icon=false}\n\n## `{r} sec`.`{r} cur` The lognormal distribution\n\nA random variable $X:\\Omega\\to{(0,\\infty)}$ has the **lognormal distribution** with parameters $\\mu$ and $\\sigma^2$ iff\n$$\n\\ln X \\sim N(\\mu,\\sigma^2).\n$$ {#eq-1.23}\nEquivalently, if $Z\\sim N(\\mu,\\sigma^2)$, then\n$X=e^Z$.\n\nNotation:\n$$\nX\\sim\\ln N (\\mu,\\sigma^2).\n$$\n\nThen\n$$\n\\mathbb{E}(X)= e^{\\mu+\\frac12\\sigma^2},\n$$ {#eq-1.24}\n$$\n\\mathrm{Var}(X)=e^{2\\mu+\\sigma^2}\\left( e^{\\sigma^2} -1\\right),\n$$ {#eq-1.25}\n$$\nf_X(x)= \\frac{1}{x\\sigma\\sqrt{2\\pi}}\\exp\\biggl(- \\frac{(\\ln x -\\mu)^2}{2\\sigma^2}\\biggr)\n$$ {#eq-1.26} \n:::\n\n::: {#fig-lognormal}\n\n```{ojs}\nviewof mu2 = Inputs.range([0, xmloggauss*0.5], { value: 0, step: 0.001, label: tex`\\mu:` })\nviewof sigma2 = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\sigma:` })\nviewof xmloggauss = Inputs.range([1, 100], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nfunction loggauss(t,mu,sigma){\n  return Math.exp(-(1/(2*sigma**2))*(Math.log(t)-mu)**2)/(t*sigma*Math.sqrt(2*Math.PI));\n}\n\nymaxloggauss = d3.max(d3.range(0.01, xmloggauss, 0.001).map(t => loggauss(t,mu2,sigma2)))\n\nPlot.plot({\n  x: { domain: [0.1,xmloggauss] },\n  y: { domain: [-ymaxloggauss*0.1,  ymaxloggauss*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    Plot.line(\n      d3\n        .range(0.1, xmloggauss, 0.01)\n        .map(t => [t, loggauss(t,mu2,sigma2)]),{\n        strokeWidth: 3,\n        stroke: \"steelblue\"\n      }\n    ),\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: 0 })\n  ]\n})\n```\n\nGraph of $f_X(x)= \\dfrac{1}{x\\sigma\\sqrt{2\\pi}}\\exp\\biggl(- \\frac{(\\ln x -\\mu)^2}{2\\sigma^2}\\biggr)$\n\n:::\n\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-note icon=false}\n\n## `{r} sec`.`{r} cur` Pareto distribution\n\nA random variable $X$ has the two-parameter Pareto distribution with parameters $\\alpha>0$ and $\\lambda>0$, if\n$$\nF_X(x)=1-\\biggl(\\frac{\\lambda}{\\lambda+x}\\biggr)^\\alpha, \\qquad x\\geq0.\n$$ {#eq-1.27}\nNotation $X\\sim Pa(\\alpha,\\lambda)$.\n\nThen\n$$\nf_X(x)=\\frac{\\alpha\\lambda^\\alpha}{(\\lambda+x)^{\\alpha+1}},\n$$ {#eq-1.28}\nand, {for $\\alpha>1$},\n$$\n\\mathbb{E}(X)=\\frac{\\lambda}{\\alpha-1}\n$$ {#eq-1.29}\nA modification of the Pareto distribution is the Burr distribution with additional parameter $\\gamma>0$\n$$\nF_{Burr}(x)=F_{Pareto}(x^\\gamma)=1-\\biggl(\\frac{\\lambda}{\\lambda+x^\\gamma}\\biggr)^\\alpha.\n$$ {#eq-1.30}\n:::\n\n::: {#fig-pareto}\n\n```{ojs}\nviewof ap = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`\\alpha:` })\nviewof lp = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`\\lambda:` })\nviewof xmpareto = Inputs.range([1, 100], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nfunction pareto(t,alpha,lambda){\n  return alpha*lambda**alpha*(t+lambda)**(-1-alpha);\n}\n\nymaxpareto = d3.max(d3.range(0.1, xmpareto, 0.01).map(t => pareto(t,ap,lp)))\n\nPlot.plot({\n  x: { domain: [0.1,xmpareto] },\n  y: { domain: [-ymaxpareto*0.1,  ymaxpareto*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    Plot.line(\n      d3\n        .range(0.1, xmpareto, 0.01)\n        .map(t => [t, pareto(t,ap,lp)]),{\n        strokeWidth: 3,\n        stroke: \"steelblue\"\n      }\n    ),\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: 0 })\n  ]\n})\n```\n\nGraph of $f_X(x)= \\dfrac{\\alpha\\lambda^\\alpha}{(\\lambda+x)^{\\alpha+1}}$\n\n:::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-note icon=false}\n\n## `{r} sec`.`{r} cur` The Weibull distribution\n\nFor $a>0$, $b>0$, we denote $X\\sim W(a,b)$ iff\n$$\nF_X(x)=1-e^{-a x^b},\n$$ {#eq-1.31}\n$$\nf_X(x)=ab x^{b-1}e^{-a x^b},\n$$ {#eq-1.32}\n$$\n\\mathbb{E}(X)=\\Gamma\\biggl(1+\\frac{1}{b}\\biggr)a^{-\\frac{1}{b}}.\n$$ {#eq-1.33}\n:::\n\n::: {#fig-weibull}\n\n```{ojs}\nviewof aw = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`a:` })\nviewof bw = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`b:` })\nviewof xmweibull = Inputs.range([1, 100], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nfunction weibull(t,a,b){\n  return a*b*t**(b-1)*Math.exp(-a*t**b);\n}\n\nymaxweibull = d3.max(d3.range(0.1, xmweibull, 0.01).map(t => weibull(t,aw,bw)))\n\nPlot.plot({\n  x: { domain: [0.1,xmweibull] },\n  y: { domain: [-ymaxweibull*0.1,  ymaxweibull*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    Plot.line(\n      d3\n        .range(0.1, xmweibull, 0.01)\n        .map(t => [t,  weibull(t,aw,bw)]),{\n        strokeWidth: 3,\n        stroke: \"steelblue\"\n      }\n    ),\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: 0 })\n  ]\n})\n```\n\nGraph of $f_X(x)= ab x^{b-1}e^{-a x^b}$\n\n:::\n\n:::: {#fig-comparison layout-nrow=3}\n\n::: {layout-ncol=2}\n\n```{ojs}\nimport {legend, swatches} from \"@d3/color-legend\"\n\n\nviewof xmin = Inputs.range([0.1, xmax*0.9], { value: 0.1, step: 0.1, label: tex`x_\\mathrm{min}` })\n\nviewof xmax = Inputs.range([1,1000], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nviewof lexp = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\lambda_{Exp}:` })\n\nviewof agamma = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\alpha_{Gamma}:` })\nviewof lgamma = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\lambda_{Gamma}:` })\n\nviewof muG = Inputs.range([0.1, 10], { value: 0, step: 0.001, label: tex`\\mu_{\\mathcal{N}}:` })\nviewof sigmaG = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\sigma_{\\mathcal{N}}:` })\n\nviewof logmu = Inputs.range([0.1, 10], { value: 0, step: 0.001, label: tex`\\mu_{\\log\\mathcal{N}}:` })\nviewof logsigma = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\sigma_{\\log\\mathcal{N}}:` })\n\nviewof apareto = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`\\alpha_{Pareto}:` })\n\nviewof lpareto = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`\\lambda_{Pareto}:` })\n\nviewof aweibull = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`a_{Weibull}:` })\n\nviewof bweibull = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`b_{Weibull}:` })\n```\n\n```{ojs}\nviewof isExp = Inputs.toggle({label: \"Exponential\",value: true})\n\nviewof isGamma = Inputs.toggle({label: \"gamma\",value: true})\n\nviewof isGauss = Inputs.toggle({label: \"normal\",value: true})\n\nviewof isLogGauss = Inputs.toggle({label: \"lognormal\",value: true})\n\nviewof isPareto = Inputs.toggle({label: \"Pareto\",value: true})\n\nviewof isWeibull = Inputs.toggle({label: \"Weibull\",value: true})\n```\n\n::: \n\n```{ojs}\n\nyexp = d3.max(d3.range(xmin, xmax, 0.01).map(t => lexp*Math.exp(-lexp * t)))\nygamma = d3.max(d3.range(xmin, xmax, 0.001).map(t => gamma(t,agamma,lgamma)))\nygauss = d3.max(d3.range(xmin, xmax, 0.01).map(t => gauss(t,mu,sigma)))\nyloggauss = d3.max(d3.range(xmin, xmax, 0.001).map(t => loggauss(t,logmu,logsigma)))\nypareto = d3.max(d3.range(xmin, xmax, 0.01).map(t => pareto(t,apareto,lpareto)))\nyweibull = d3.max(d3.range(xmin, xmax, 0.01).map(t => weibull(t,aweibull,bweibull)))\n\ntfull = d3.max([isExp?yexp:0, isGamma?ygamma:0, isGauss?ygauss:0, isPareto?ypareto:0, isWeibull?yweibull:0, isLogGauss?yloggauss:0])\n\nPlot.plot({\n  x: { domain: [xmin, xmax] },\n  y: { domain: [-tfull*0.1,  tfull*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    (isExp == true)?Plot.line(\n      d3\n        .range(xmin, xmax, 0.01)\n        .map(t => [t, lexp*Math.exp(-lexp*t)]),{\n        strokeWidth: 3,\n        stroke: d3.schemeCategory10[0]\n      }\n    ):Plot.line(\n      d3\n        .range(0, 0.01, 0.01)\n        .map(t => [t, t]),{\n        strokeWidth: 0,\n        stroke: d3.schemeCategory10[0]\n      }\n    ),\n    (isGamma == true)?Plot.line(\n      d3\n        .range(xmin, xmax, 0.01)\n        .map(t => [t, gamma(t,agamma,lgamma)]),{\n        strokeWidth: 3,\n        stroke: d3.schemeCategory10[1]\n      }\n    ):Plot.line(\n      d3\n        .range(0, 0.01, 0.01)\n        .map(t => [t, t]),{\n        strokeWidth: 0,\n        stroke: d3.schemeCategory10[0]\n      }\n    ),\n    (isGauss == true)?Plot.line(\n      d3\n        .range(xmin, xmax, 0.01)\n        .map(t => [t, gauss(t,muG,sigmaG)]),{\n        strokeWidth: 3,\n        stroke: d3.schemeCategory10[2]\n      }\n    ):Plot.line(\n      d3\n        .range(0, 0.01, 0.01)\n        .map(t => [t, t]),{\n        strokeWidth: 0,\n        stroke: d3.schemeCategory10[0]\n      }\n    ),\n    (isLogGauss == true)?Plot.line(\n      d3\n        .range(xmin, xmax, 0.01)\n        .map(t => [t, loggauss(t,logmu,logsigma)]),{\n        strokeWidth: 3,\n        stroke: d3.schemeCategory10[3]\n      }\n    ):Plot.line(\n      d3\n        .range(0, 0.01, 0.01)\n        .map(t => [t, t]),{\n        strokeWidth: 0,\n        stroke: d3.schemeCategory10[0]\n      }\n    ),\n    (isPareto == true)?Plot.line(\n      d3\n        .range(xmin, xmax, 0.01)\n        .map(t => [t, pareto(t,apareto,lpareto)]),{\n        strokeWidth: 3,\n        stroke: d3.schemeCategory10[4]\n      }\n    ):Plot.line(\n      d3\n        .range(0, 0.01, 0.01)\n        .map(t => [t, t]),{\n        strokeWidth: 0,\n        stroke: d3.schemeCategory10[0]\n      }\n    ),\n    (isWeibull == true)?Plot.line(\n      d3\n        .range(xmin, xmax, 0.01)\n        .map(t => [t,  weibull(t,aweibull,bweibull)]),{\n        strokeWidth: 3,\n        stroke: d3.schemeCategory10[5]\n      }\n    ):Plot.line(\n      d3\n        .range(0, 0.01, 0.01)\n        .map(t => [t, t]),{\n        strokeWidth: 0,\n        stroke: d3.schemeCategory10[0]\n      }\n    ),\n      Plot.ruleX([xmin]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: xmin })\n  ]\n})\n```\n\n\n```{ojs}\n\nswatches({\n  color: d3.scaleOrdinal([tex`\\displaystyle f_X(x)=\\lambda e^{-\\lambda x}`, tex`\\displaystyle f_X(x)=\\frac{\\lambda^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1} e^{-\\lambda x}`, tex`\\displaystyle f_X(x)=\\frac1{\\sigma\\sqrt{2\\pi}}e^{-\\frac1{2\\sigma^2}(x-\\mu)^2}`, tex`\\displaystyle f_X(x)= \\frac{1}{x\\sigma\\sqrt{2\\pi}}\\exp\\biggl(- \\frac{(\\ln x -\\mu)^2}{2\\sigma^2}\\biggr)`, tex`\\displaystyle f_X(x)=\\frac{\\alpha\\lambda^\\alpha}{(\\lambda+x)^{\\alpha+1}}`, tex`\\displaystyle f_X(x)=ab x^{b-1}e^{-a x^b}`], d3.schemeCategory10)\n    })\n```\n\nComparison of right tails for the density functions\n\n::::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-note icon=false}\n\n## `{r} sec`.`{r} cur` Cheracterisations\n\nA random variable is uniquely determined by either of CDF, PDF, MGF. Note that the sequence of all moments, in general, does not determine the distribution uniquely (Hamburger moment problem).\n\nRelations:\n$$\nf_X(x) = F'_X(x), \n$$ {#eq-1.34}\n$$\nF_X(x)  = \\int_{-\\infty}^x f_x(y)\\,dy,\n$$ {#eq-1.35}\n$$\nm_{k,X} = M_X^{(k)}(0).\n$$ {#eq-1.36}\n:::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-note icon=false}\n\n## `{r} sec`.`{r} cur` Likelihood\n\nLet $X:\\Omega\\to\\mathbb{R}$ be a random variable whose distribution depends on a parameter $\\theta\\in\\mathbb{R}$.\n\nSuppose that we observe the data $x_1,\\ldots,x_n$ which is the output of this random variable $X$ in course of $n$ independent trials.\n\nIn other words, we can say that we observe that i.i.d.r.v. $X_1,\\ldots, X_n$ with $X_i\\sim X$, $1\\leq i\\leq n$, take certain values: $X_1=x_1,\\ldots, X_n=x_n$.\n\nThe **likelihood**, or **likelihood function**, is the function $\\mathcal{L}(\\theta)=\\mathcal{L}(\\theta\\mid x_1,\\ldots,x_n)$ of the unknown parameter $\\theta$ (given the observed data $x_1,\\ldots,x_n$) which is equal to:\n\n- (if $X$ is a discrete random variable)\nthe probability to observe this data (given the value of the parameter $\\theta$):\n$$\n\\begin{aligned}\n\\mathcal{L}(\\theta)&=\\mathcal{L}(\\theta\\mid x_1,\\ldots,x_n)\\\\\n:&=\\mathbb{P}(X_1=x_1,\\ldots,X_n=x_n\\mid \\theta)\n\\\\&= \\mathbb{P}(X_1=x_1\\mid \\theta)\\ldots \\mathbb{P}(X_n=x_n\\mid \\theta).\n\\end{aligned}\n$$ {#eq-1.37}\n\n- (if $X$ is a continuous random variable with the PDF $f_X(x)=f_X(x\\mid\\theta)$)\n$$\n\\begin{aligned}\n\\mathcal{L}(\\theta)&=\\mathcal{L}(\\theta\\mid x_1,\\ldots,x_n)\\\\\n:&=f_X(x_1\\mid\\theta)f_X(x_2\\mid\\theta)\\ldots f_X(x_n\\mid\\theta).\n\\end{aligned}\n$$ {#eq-1.38}\n\n:::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-note icon=false}\n\n## `{r} sec`.`{r} cur` Maximum likelihood estimator\n\nThe **maximum likelihood estimator**$\\theta_*$ of the parameter $\\theta$ is the argument of the maximum of the likelihood function:\n$$\n\\theta_*=\\mathop{\\mathrm{argmax}}_\\theta\\mathcal{L}(\\theta),\n$$ {#eq-1.39}\nthat means that\n$$\n\\mathcal{L}(\\theta_*) = \\max_{\\theta}\\mathcal{L}(\\theta).\n$$ {#eq-1.40}\n\nThe standard approach to find $\\theta_*$ is to consider the **log-likelihood function**\n$$\nL(\\theta):=L(\\theta\\mid x_1,\\ldots,x_n)=\\ln \\mathcal{L}(\\theta\\mid x_1,\\ldots,x_n).\n$$ {#eq-1.41}\n\nThus, in the discrete case,\n$$\nL(\\theta) = \\ln \\mathbb{P}(X_1=x_1\\mid \\theta)+ \\ldots + \\ln \\mathbb{P}(X_n=x_n\\mid \\theta),\n$$ {#eq-1.42}\nwhereas, in the continuous case\n$$\nL(\\theta) = \\ln f_X(x_1\\mid\\theta)+ \\ldots + \\ln f_X(x_n\\mid\\theta),\n$$ {#eq-1.43}\n\nThen $\\theta_*$ is the point of maximum for both $\\mathcal{L}$ and $L$:\n$$\n\\theta_*=\\mathop{\\mathrm{argmax}}_\\theta L(\\theta)=\\mathop{\\mathrm{argmax}}_\\theta\\mathcal{L}(\\theta).\n$$ {#eq-1.44}\n:::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-warning icon=false}\n\n## `{r} sec`.`{r} cur` Example: Discrete case\n\nLet $X\\sim Po(\\lambda)$, i.e.\n$$\n\\mathbb{P}(X=k)=\\frac{\\lambda^k}{k!}e^{-\\lambda}, \\quad k\\geq0.\n$$\nSuppose we have a sample of $n$ values of $X$: $k_1,\\ldots,k_n$, and we want to estimate $\\lambda$ (here $\\theta=\\lambda$). Then\n$$\n\\begin{aligned}\n\\mathcal{L}(\\lambda)&=\\mathbb{P}(X=k_1\\mid\\lambda)\\ldots \\mathbb{P}(X=k_n\\mid\\lambda)\\\\\n& = \\frac{\\lambda^{k_1}}{k_1!}e^{-\\lambda}\\cdot\\ldots\\cdot \\frac{\\lambda^{k_n}}{k_n!}e^{-\\lambda}\\\\\n& = \\underbrace{\\frac{1}{k_1!\\ldots k_n!}}_{=: c>0}\\lambda^{k_1+\\ldots+k_n}e^{-\\lambda n},\n\\end{aligned}\n$$\nand therefore,\n$$\n\\begin{aligned}\nL(\\lambda)&=\\ln\\mathcal{L}(\\lambda)\n\\\\& = \\ln c + (k_1+\\ldots+k_n)\\ln\\lambda-\\lambda n.\n\\end{aligned}\n$$\nThen\n$$\nL'(\\lambda) = \\frac{k_1+\\ldots+k_n}{\\lambda}-n,\n$$\nand hence, $L'(\\lambda)=0$ iff\n$$\n{\\lambda = \\frac{k_1+\\ldots+k_n}{n}}.\n$$\n\nSince\n$$\nL''(\\lambda) = (L'(\\lambda))'=-\\frac{k_1+\\ldots+k_n}{\\lambda^2}<0,\n$$\nthe found value $\\lambda_* = \\frac{k_1+\\ldots+k_n}{n}$ is the point of maximum of $L$, hence, it is the maximum likelihood estimator for the parameter $\\lambda$.\n:::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-tip icon=false}\n\n## `{r} sec`.`{r} cur` Remark:  Relation to the LLN\n\nNote that, by the law of large numbers (LLN), if $X_i\\sim X\\sim Po(\\lambda)$, $i\\in\\mathbb{N}$, then\n$$\n\\frac{X_1+\\ldots+X_n}{n} \\to \\mathbb{E}(X)=\\lambda, \\qquad n\\to\\infty.\n$$\nIn other words, the maximum likelihood estimator converges to the theoretical value is the size of the sample converges to infinity.\n:::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-warning icon=false}\n\n## `{r} sec`.`{r} cur` Example: Continuous case\n\nLet $X\\sim Exp(\\lambda)$. Suppose we have a sample of $n$ values of $X$: $x_1,\\ldots,x_n$. Then\n$$\n\\begin{aligned}\n\\mathcal{L}(\\lambda) &= f_X(x_1\\mid \\lambda)\\ldots f_X(x_n\\mid \\lambda)\\\\\n&=\\lambda e^{-\\lambda x_1}\\ldots \\lambda e^{-\\lambda x_n}= \\lambda^n e^{-\\lambda(x_1+\\ldots +x_n)}.\n\\end{aligned}\n$$\n\nTherefore,\n$$\n\\begin{aligned}\nL(\\lambda)&= \\ln \\mathcal L(\\lambda)\n\\\\&= n \\ln \\lambda - \\lambda(x_1+\\ldots +x_n),\n\\end{aligned}\n$$\nso $L'(\\lambda)=0$ iff\n$$\n\\begin{gathered}\n\\frac{n}{\\lambda} -(x_1+\\ldots +x_n)=0,\\\\\n{\\lambda_*= \\frac{n}{x_1+\\ldots +x_n}}.\n\\end{gathered}\n$$\nNote that $L''(\\lambda)=-\\dfrac{n}{\\lambda^2}$, in particular, $L''(\\lambda_*)<0$, i.e. $\\lambda_*$ is indeed the maximum likelihood estimator for the parameter $\\lambda$.\n\nFinally, note that if $X_i\\sim X\\sim Exp(\\lambda)$, then, by LLN,\n$$\n\\frac{X_1+\\ldots+X_n}{n}\\to \\mathbb{E}(X)=\\frac{1}{\\lambda}, \\qquad n\\to\\infty.\n$$\n:::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-note icon=false}\n\n## `{r} sec`.`{r} cur` The method of moments\n\nIf the number of parameters is bigger than $1$, the maximum likelihood estimation may be challenging as one would need to find the point of maximum for a function of several variables (though it can be still effectively done numerically).\n\nAnother method to estimate unknown parameters of the distribution is the **method of moments**. Namely, assuming that we have a sample of $n$ values $x_1,\\ldots,x_n$ of a random variable $X$ which is believed to be followed a probability distribution which depends on $k$ unknown parameters $\\theta=(\\theta_1,\\ldots,\\theta_k)$.\n\nThen, we consider the first $k$ moments of the random variable $X$ (i.e. the moments of the population):\n$$\nm_j = \\mathbb{E}(X^j\\mid \\theta), \\qquad 1\\leq j\\leq k.\n$$\n\nNext, for the given data $x_1,\\ldots,x_n$, we consider $k$ averaged moments (i.e. the moments of the sample):\n$$\n\\overline{m}_j = \\frac{1}{n}\\sum_{l=1}^n x_l ^j, \\qquad 1\\leq j\\leq k.\n$$\n\nAfter this, we need to equate $k$ quantities:\n$$\nm_j = \\overline{m}_j, \\qquad 1\\leq j\\leq k,\n$$\nto determine $k$ unknown parameters $\\theta_1,\\ldots,\\theta_k$.\n:::\n\n","srcMarkdownNoYaml":"\n\n```{ojs}\n//| output: false\nmathfn = require('https://bundle.run/mathfn@1.1.0')\n\n```\n\n```{r}\n#| echo: false\nsec = 1\ncur = 0\n```\n\n# Loss distributions\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-note icon=false}\n\n## `{r} sec`.`{r} cur` Probability distribution\n\n- Let $X:\\Omega\\to\\mathbb{R}$ be a continuous random variable.\n\n- The **cummulative distribution function (CDF)** of $X$ is a **non-decreasing** function\n$$\nF_X(x)=\\mathbb{P}(X\\leq x)\\in[0,1].\n$$ {#eq-1.1}\n\n- The **probability density function (PDF)** (a.k.a. **probability density**) of $X$ is\n$$\nf_X(x)=F'_X(x)\\geq0.\n$$ {#eq-1.2}\n\n- Relation:\n$$\n\\mathbb{P}(a\\leq X \\leq b) = \\int_a^b f_X(x)\\,dx=F_X(b)-F_X(a).\n$$ {#eq-1.3}\n\n- For a $k\\in\\mathbb{N}$, the **moment of order $k$** of $X$ is\n$$\nm_k=m_{k,X}=\\mathbb{E}(X^k)=\\int_\\mathbb{R} x^k \\, f_X(x)\\,dx.\n$$ {#eq-kthmoment}\n\n- The **moment generator function (MGM)** is\n$$\nM_X(t)=\\mathbb{E}(e^{tX})=\\int_\\mathbb{R} e^{tx} \\, f_X(x)\\,dx\\geq0.\n$$ {#eq-1.5}\n\n- Relation:\n$$\nM_X(t)=1+\\sum_{k=1}^\\infty\\frac{m_k}{k!}t^k.\n$$ {#eq-1.6}\n\n:::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-tip icon=false}\n\n## `{r} sec`.`{r} cur` Remark\n\n- Recall that functions $\\dfrac{1}{1+|x|^\\beta}$ and $\\dfrac{1}{(1+|x|)^\\beta}$ are integrable on $\\mathbb{R}$ iff $\\beta>1$.\n\n- Note that if $f_X(x)$ is continuous on $\\mathbb{R}$, it's integrable on arbitrary large interval $[-r,r]$, $r>0$. Therefore, if there exists $A>$ and $r>0$ such that\n$$\n|f_X(x)|\\leq \\frac{A}{1+|x|^\\beta}, \\qquad |x|>r,\n$$ {#eq-sufcondint}\nand $\\beta>1$, then indeed, $\\int\\limits_{\\mathbb{R}}|f_X(x)|\\,dx<\\infty$.\n\n- As a result, if\n$\\beta>k+1$ in ([-@eq-sufcondint]), then the $k$-th moment $m_k$ in ([-@eq-kthmoment]) is well defined and finite.\n\n- Examples of such probability densities $f_X(x)$ may be, apart from the obvious $\\dfrac{1}{1+|x|^\\beta}$ and $\\dfrac{1}{(1+|x|)^\\beta}$ with $\\beta>k+1$, also the considered below Weibull function\n$$\nf_X(x)=e^{-a |x|^b}\n$$ {#eq-Weibull}\nwith **any** positive $a,b>0$.\n\n- Moreover, if in ([-@eq-Weibull])\n$b>1$, then\n$0< M_X(t)<\\infty$.\n\n:::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-note icon=false}\n\n## `{r} sec`.`{r} cur` The exponential distribution\n\nA random variable $X:\\Omega\\to\\mathbb{R}$ has the **exponential distribution** with parameter $\\lambda>0$ if its CDF is\n$$\nF(x)=1-e^{-\\lambda x}, \\quad x\\geq0.\n$$ {#eq-1.9}\n\nHere and below, when we write a restriction on $x$, we mean that function is $0$ otherwise, i.e. here $F(x)=0$ for $x<0$.\n\nThen the PDF is\n$$\nf(x)=\\lambda e^{-\\lambda x}, \\quad x\\geq0.\n$$ {#eq-1.10}\n\nNext,\n$$\n\\mathbb{E}(X)=\\frac1{\\lambda},\n$$ {#eq-1.11} \n$$\n\\mathrm{Var}(X)=\\frac1{\\lambda^2}= \\bigl( \\mathbb{E}(X) \\bigr)^2,\n$$ {#eq-1.12}\n$$\n\\mathbb{E}(X^2)=\\frac2{\\lambda^2}=2\\bigl( \\mathbb{E}(X) \\bigr)^2.\n$$ {#eq-1.13}\nThe MGM if defined for $t<\\lambda$ only:\n$$\nM(t)=\\biggl(1-\\frac{t}{\\lambda}\\biggr)^{-1}=\\frac{\\lambda}{\\lambda-t}.\n$$\nThe notation is\n$$\nX\\sim Exp(\\lambda).\n$$ {#eq-1.14}\n\n:::\n\n::: {#fig-Exp}\n\n```{ojs}\n\nviewof l = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\lambda:` })\n\nviewof xmexp = Inputs.range([1, 100], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nymaxexp = d3.max(d3.range(0, xmexp, 0.01).map(t => l*Math.exp(-l * t)))\n\nPlot.plot({\n  x: { domain: [-0.1, xmexp] },\n  y: { domain: [-0.1*ymaxexp, ymaxexp*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    Plot.line(\n      d3\n        .range(0, xmexp, 0.1)\n        .map(t => [\n          t,\n          l*Math.exp(-l * t)\n        ]),{\n        strokeWidth: 3,\n        stroke: \"steelblue\"\n      }\n    ),\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: 0 })\n  ]\n})\n```\n\nGraph of $f(x)=\\lambda e^{-\\lambda x}$\n:::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-note icon=false}\n\n## `{r} sec`.`{r} cur` The gamma distribution\n\nA random variable $X:\\Omega\\to\\mathbb{R}$ has the **gamma distribution** with parameters $\\alpha>0$ and $\\lambda>0$ if its PDF is\n$$\nf(x)=\\frac{\\lambda^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1} e^{-\\lambda x}, \\quad x\\geq0.\n$$ {#eq-1.15}\nHere\n$$\n\\Gamma(\\alpha):=\\int_0^\\infty t^{\\alpha-1}e^{-t}\\,dt, \\quad \\alpha>0\n$$ {#eq-1.16}\nis the gamma function.\n\nThe following formulas hold:\n$$\n\\mathbb{E}(X)=\\frac{\\alpha}{\\lambda},\n$$ {#eq-1.17}\n$$\n\\mathrm{Var}(X)=\\frac{\\alpha}{\\lambda^2},\n$$ {#eq-1.18}\n$$\nM_X(t)=\\biggl(1-\\frac{t}{\\lambda}\\biggr)^{-\\alpha}=\\frac{\\lambda^\\alpha}{(\\lambda-t)^\\alpha}, \\quad t<\\lambda.\n$$ {#eq-1.19}\nThe notation is\n$$\nX\\sim \\Gamma(\\alpha,\\lambda).\n$$\n\n:::\n\n::: {#fig-gamma}\n\n```{ojs}\nviewof a = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\alpha:` })\nviewof l2 = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\lambda:` })\nviewof xmgamma = Inputs.range([1, 100], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nfunction gamma(t,alpha,lambda){\n  return ((lambda**alpha)/mathfn.gamma(alpha))*(t**(alpha-1))*Math.exp(-lambda * t);\n}\n\n\nymaxgamma = d3.max(d3.range(0.1, xmgamma, 0.001).map(t => gamma(t,a,l2)))\n\n\nPlot.plot({\n  x: { domain: [0.1, xmgamma] },\n  y: { domain: [-0.1*ymaxgamma, ymaxgamma*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    Plot.line(\n      d3\n        .range(0,xmgamma, 0.01)\n        .map(t => [t, gamma(t,a,l2)]),{\n        strokeWidth: 3,\n        stroke: \"steelblue\"\n      }\n    ),\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: 0 })\n  ]\n})\n```\n\nGraph of $f(x)=\\dfrac{\\lambda^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1} e^{-\\lambda x}$\n\n:::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-note icon=false}\n\n## `{r} sec`.`{r} cur` The normal distribution\n\nA random variable $X:\\Omega\\to\\mathbb{R}$ has the **normal distribution** with the mean $\\mu$ and the variance $\\sigma^2$ if\n$$\nf_X(x)=\\frac1{\\sigma\\sqrt{2\\pi}}e^{-\\frac1{2\\sigma^2}(x-\\mu)^2}. \n$$ {#eq-1.20}\nIndeed,\n$$\n\\mathbb{E}(X)=\\mu, \\quad \\mathrm{Var}(X)=\\sigma^2.\n$$ {#eq-1.21}\nThe MGF is defined now everywhere:\n$$\nM_X(t)=e^{\\mu t +\\frac12 \\sigma^2 t^2}, \\quad t\\in\\mathbb{R}.\n$$ {#eq-1.22}\nNotation\n$$\nX\\sim N(\\mu,\\sigma^2).\n$$\n:::\n\n::: {#fig-normal}\n\n```{ojs}\nviewof mu = Inputs.range([-0.7*xmgauss, 0.7*xmgauss], { value: 0, step: 0.001, label: tex`\\mu:` })\nviewof sigma = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\sigma:` })\nviewof xmgauss = Inputs.range([1, 100], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nfunction gauss(t,mu,sigma){\n  return Math.exp(-(1/(2*sigma**2))*(t-mu)**2)/(sigma*Math.sqrt(2*Math.PI));\n}\n\nymaxgauss = d3.max(d3.range(-xmgauss, xmgauss, 0.01).map(t => gauss(t,mu,sigma)))\n\nPlot.plot({\n  x: { domain: [-xmgauss, xmgauss] },\n  y: { domain: [-ymaxgauss*0.1, ymaxgauss*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    Plot.line(\n      d3\n        .range(-xmgauss, xmgauss, 0.01)\n        .map(t => [t, gauss(t,mu,sigma)]),{\n        strokeWidth: 3,\n        stroke: \"steelblue\"\n      }\n    ),\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: 0 })\n  ]\n})\n```\n\nGraph of $f_X(x)=\\dfrac1{\\sigma\\sqrt{2\\pi}}e^{-\\frac1{2\\sigma^2}(x-\\mu)^2}$\n\n:::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-note icon=false}\n\n## `{r} sec`.`{r} cur` The lognormal distribution\n\nA random variable $X:\\Omega\\to{(0,\\infty)}$ has the **lognormal distribution** with parameters $\\mu$ and $\\sigma^2$ iff\n$$\n\\ln X \\sim N(\\mu,\\sigma^2).\n$$ {#eq-1.23}\nEquivalently, if $Z\\sim N(\\mu,\\sigma^2)$, then\n$X=e^Z$.\n\nNotation:\n$$\nX\\sim\\ln N (\\mu,\\sigma^2).\n$$\n\nThen\n$$\n\\mathbb{E}(X)= e^{\\mu+\\frac12\\sigma^2},\n$$ {#eq-1.24}\n$$\n\\mathrm{Var}(X)=e^{2\\mu+\\sigma^2}\\left( e^{\\sigma^2} -1\\right),\n$$ {#eq-1.25}\n$$\nf_X(x)= \\frac{1}{x\\sigma\\sqrt{2\\pi}}\\exp\\biggl(- \\frac{(\\ln x -\\mu)^2}{2\\sigma^2}\\biggr)\n$$ {#eq-1.26} \n:::\n\n::: {#fig-lognormal}\n\n```{ojs}\nviewof mu2 = Inputs.range([0, xmloggauss*0.5], { value: 0, step: 0.001, label: tex`\\mu:` })\nviewof sigma2 = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\sigma:` })\nviewof xmloggauss = Inputs.range([1, 100], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nfunction loggauss(t,mu,sigma){\n  return Math.exp(-(1/(2*sigma**2))*(Math.log(t)-mu)**2)/(t*sigma*Math.sqrt(2*Math.PI));\n}\n\nymaxloggauss = d3.max(d3.range(0.01, xmloggauss, 0.001).map(t => loggauss(t,mu2,sigma2)))\n\nPlot.plot({\n  x: { domain: [0.1,xmloggauss] },\n  y: { domain: [-ymaxloggauss*0.1,  ymaxloggauss*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    Plot.line(\n      d3\n        .range(0.1, xmloggauss, 0.01)\n        .map(t => [t, loggauss(t,mu2,sigma2)]),{\n        strokeWidth: 3,\n        stroke: \"steelblue\"\n      }\n    ),\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: 0 })\n  ]\n})\n```\n\nGraph of $f_X(x)= \\dfrac{1}{x\\sigma\\sqrt{2\\pi}}\\exp\\biggl(- \\frac{(\\ln x -\\mu)^2}{2\\sigma^2}\\biggr)$\n\n:::\n\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-note icon=false}\n\n## `{r} sec`.`{r} cur` Pareto distribution\n\nA random variable $X$ has the two-parameter Pareto distribution with parameters $\\alpha>0$ and $\\lambda>0$, if\n$$\nF_X(x)=1-\\biggl(\\frac{\\lambda}{\\lambda+x}\\biggr)^\\alpha, \\qquad x\\geq0.\n$$ {#eq-1.27}\nNotation $X\\sim Pa(\\alpha,\\lambda)$.\n\nThen\n$$\nf_X(x)=\\frac{\\alpha\\lambda^\\alpha}{(\\lambda+x)^{\\alpha+1}},\n$$ {#eq-1.28}\nand, {for $\\alpha>1$},\n$$\n\\mathbb{E}(X)=\\frac{\\lambda}{\\alpha-1}\n$$ {#eq-1.29}\nA modification of the Pareto distribution is the Burr distribution with additional parameter $\\gamma>0$\n$$\nF_{Burr}(x)=F_{Pareto}(x^\\gamma)=1-\\biggl(\\frac{\\lambda}{\\lambda+x^\\gamma}\\biggr)^\\alpha.\n$$ {#eq-1.30}\n:::\n\n::: {#fig-pareto}\n\n```{ojs}\nviewof ap = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`\\alpha:` })\nviewof lp = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`\\lambda:` })\nviewof xmpareto = Inputs.range([1, 100], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nfunction pareto(t,alpha,lambda){\n  return alpha*lambda**alpha*(t+lambda)**(-1-alpha);\n}\n\nymaxpareto = d3.max(d3.range(0.1, xmpareto, 0.01).map(t => pareto(t,ap,lp)))\n\nPlot.plot({\n  x: { domain: [0.1,xmpareto] },\n  y: { domain: [-ymaxpareto*0.1,  ymaxpareto*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    Plot.line(\n      d3\n        .range(0.1, xmpareto, 0.01)\n        .map(t => [t, pareto(t,ap,lp)]),{\n        strokeWidth: 3,\n        stroke: \"steelblue\"\n      }\n    ),\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: 0 })\n  ]\n})\n```\n\nGraph of $f_X(x)= \\dfrac{\\alpha\\lambda^\\alpha}{(\\lambda+x)^{\\alpha+1}}$\n\n:::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-note icon=false}\n\n## `{r} sec`.`{r} cur` The Weibull distribution\n\nFor $a>0$, $b>0$, we denote $X\\sim W(a,b)$ iff\n$$\nF_X(x)=1-e^{-a x^b},\n$$ {#eq-1.31}\n$$\nf_X(x)=ab x^{b-1}e^{-a x^b},\n$$ {#eq-1.32}\n$$\n\\mathbb{E}(X)=\\Gamma\\biggl(1+\\frac{1}{b}\\biggr)a^{-\\frac{1}{b}}.\n$$ {#eq-1.33}\n:::\n\n::: {#fig-weibull}\n\n```{ojs}\nviewof aw = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`a:` })\nviewof bw = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`b:` })\nviewof xmweibull = Inputs.range([1, 100], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nfunction weibull(t,a,b){\n  return a*b*t**(b-1)*Math.exp(-a*t**b);\n}\n\nymaxweibull = d3.max(d3.range(0.1, xmweibull, 0.01).map(t => weibull(t,aw,bw)))\n\nPlot.plot({\n  x: { domain: [0.1,xmweibull] },\n  y: { domain: [-ymaxweibull*0.1,  ymaxweibull*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    Plot.line(\n      d3\n        .range(0.1, xmweibull, 0.01)\n        .map(t => [t,  weibull(t,aw,bw)]),{\n        strokeWidth: 3,\n        stroke: \"steelblue\"\n      }\n    ),\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: 0 })\n  ]\n})\n```\n\nGraph of $f_X(x)= ab x^{b-1}e^{-a x^b}$\n\n:::\n\n:::: {#fig-comparison layout-nrow=3}\n\n::: {layout-ncol=2}\n\n```{ojs}\nimport {legend, swatches} from \"@d3/color-legend\"\n\n\nviewof xmin = Inputs.range([0.1, xmax*0.9], { value: 0.1, step: 0.1, label: tex`x_\\mathrm{min}` })\n\nviewof xmax = Inputs.range([1,1000], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nviewof lexp = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\lambda_{Exp}:` })\n\nviewof agamma = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\alpha_{Gamma}:` })\nviewof lgamma = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\lambda_{Gamma}:` })\n\nviewof muG = Inputs.range([0.1, 10], { value: 0, step: 0.001, label: tex`\\mu_{\\mathcal{N}}:` })\nviewof sigmaG = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\sigma_{\\mathcal{N}}:` })\n\nviewof logmu = Inputs.range([0.1, 10], { value: 0, step: 0.001, label: tex`\\mu_{\\log\\mathcal{N}}:` })\nviewof logsigma = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\sigma_{\\log\\mathcal{N}}:` })\n\nviewof apareto = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`\\alpha_{Pareto}:` })\n\nviewof lpareto = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`\\lambda_{Pareto}:` })\n\nviewof aweibull = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`a_{Weibull}:` })\n\nviewof bweibull = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`b_{Weibull}:` })\n```\n\n```{ojs}\nviewof isExp = Inputs.toggle({label: \"Exponential\",value: true})\n\nviewof isGamma = Inputs.toggle({label: \"gamma\",value: true})\n\nviewof isGauss = Inputs.toggle({label: \"normal\",value: true})\n\nviewof isLogGauss = Inputs.toggle({label: \"lognormal\",value: true})\n\nviewof isPareto = Inputs.toggle({label: \"Pareto\",value: true})\n\nviewof isWeibull = Inputs.toggle({label: \"Weibull\",value: true})\n```\n\n::: \n\n```{ojs}\n\nyexp = d3.max(d3.range(xmin, xmax, 0.01).map(t => lexp*Math.exp(-lexp * t)))\nygamma = d3.max(d3.range(xmin, xmax, 0.001).map(t => gamma(t,agamma,lgamma)))\nygauss = d3.max(d3.range(xmin, xmax, 0.01).map(t => gauss(t,mu,sigma)))\nyloggauss = d3.max(d3.range(xmin, xmax, 0.001).map(t => loggauss(t,logmu,logsigma)))\nypareto = d3.max(d3.range(xmin, xmax, 0.01).map(t => pareto(t,apareto,lpareto)))\nyweibull = d3.max(d3.range(xmin, xmax, 0.01).map(t => weibull(t,aweibull,bweibull)))\n\ntfull = d3.max([isExp?yexp:0, isGamma?ygamma:0, isGauss?ygauss:0, isPareto?ypareto:0, isWeibull?yweibull:0, isLogGauss?yloggauss:0])\n\nPlot.plot({\n  x: { domain: [xmin, xmax] },\n  y: { domain: [-tfull*0.1,  tfull*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    (isExp == true)?Plot.line(\n      d3\n        .range(xmin, xmax, 0.01)\n        .map(t => [t, lexp*Math.exp(-lexp*t)]),{\n        strokeWidth: 3,\n        stroke: d3.schemeCategory10[0]\n      }\n    ):Plot.line(\n      d3\n        .range(0, 0.01, 0.01)\n        .map(t => [t, t]),{\n        strokeWidth: 0,\n        stroke: d3.schemeCategory10[0]\n      }\n    ),\n    (isGamma == true)?Plot.line(\n      d3\n        .range(xmin, xmax, 0.01)\n        .map(t => [t, gamma(t,agamma,lgamma)]),{\n        strokeWidth: 3,\n        stroke: d3.schemeCategory10[1]\n      }\n    ):Plot.line(\n      d3\n        .range(0, 0.01, 0.01)\n        .map(t => [t, t]),{\n        strokeWidth: 0,\n        stroke: d3.schemeCategory10[0]\n      }\n    ),\n    (isGauss == true)?Plot.line(\n      d3\n        .range(xmin, xmax, 0.01)\n        .map(t => [t, gauss(t,muG,sigmaG)]),{\n        strokeWidth: 3,\n        stroke: d3.schemeCategory10[2]\n      }\n    ):Plot.line(\n      d3\n        .range(0, 0.01, 0.01)\n        .map(t => [t, t]),{\n        strokeWidth: 0,\n        stroke: d3.schemeCategory10[0]\n      }\n    ),\n    (isLogGauss == true)?Plot.line(\n      d3\n        .range(xmin, xmax, 0.01)\n        .map(t => [t, loggauss(t,logmu,logsigma)]),{\n        strokeWidth: 3,\n        stroke: d3.schemeCategory10[3]\n      }\n    ):Plot.line(\n      d3\n        .range(0, 0.01, 0.01)\n        .map(t => [t, t]),{\n        strokeWidth: 0,\n        stroke: d3.schemeCategory10[0]\n      }\n    ),\n    (isPareto == true)?Plot.line(\n      d3\n        .range(xmin, xmax, 0.01)\n        .map(t => [t, pareto(t,apareto,lpareto)]),{\n        strokeWidth: 3,\n        stroke: d3.schemeCategory10[4]\n      }\n    ):Plot.line(\n      d3\n        .range(0, 0.01, 0.01)\n        .map(t => [t, t]),{\n        strokeWidth: 0,\n        stroke: d3.schemeCategory10[0]\n      }\n    ),\n    (isWeibull == true)?Plot.line(\n      d3\n        .range(xmin, xmax, 0.01)\n        .map(t => [t,  weibull(t,aweibull,bweibull)]),{\n        strokeWidth: 3,\n        stroke: d3.schemeCategory10[5]\n      }\n    ):Plot.line(\n      d3\n        .range(0, 0.01, 0.01)\n        .map(t => [t, t]),{\n        strokeWidth: 0,\n        stroke: d3.schemeCategory10[0]\n      }\n    ),\n      Plot.ruleX([xmin]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: xmin })\n  ]\n})\n```\n\n\n```{ojs}\n\nswatches({\n  color: d3.scaleOrdinal([tex`\\displaystyle f_X(x)=\\lambda e^{-\\lambda x}`, tex`\\displaystyle f_X(x)=\\frac{\\lambda^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1} e^{-\\lambda x}`, tex`\\displaystyle f_X(x)=\\frac1{\\sigma\\sqrt{2\\pi}}e^{-\\frac1{2\\sigma^2}(x-\\mu)^2}`, tex`\\displaystyle f_X(x)= \\frac{1}{x\\sigma\\sqrt{2\\pi}}\\exp\\biggl(- \\frac{(\\ln x -\\mu)^2}{2\\sigma^2}\\biggr)`, tex`\\displaystyle f_X(x)=\\frac{\\alpha\\lambda^\\alpha}{(\\lambda+x)^{\\alpha+1}}`, tex`\\displaystyle f_X(x)=ab x^{b-1}e^{-a x^b}`], d3.schemeCategory10)\n    })\n```\n\nComparison of right tails for the density functions\n\n::::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-note icon=false}\n\n## `{r} sec`.`{r} cur` Cheracterisations\n\nA random variable is uniquely determined by either of CDF, PDF, MGF. Note that the sequence of all moments, in general, does not determine the distribution uniquely (Hamburger moment problem).\n\nRelations:\n$$\nf_X(x) = F'_X(x), \n$$ {#eq-1.34}\n$$\nF_X(x)  = \\int_{-\\infty}^x f_x(y)\\,dy,\n$$ {#eq-1.35}\n$$\nm_{k,X} = M_X^{(k)}(0).\n$$ {#eq-1.36}\n:::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-note icon=false}\n\n## `{r} sec`.`{r} cur` Likelihood\n\nLet $X:\\Omega\\to\\mathbb{R}$ be a random variable whose distribution depends on a parameter $\\theta\\in\\mathbb{R}$.\n\nSuppose that we observe the data $x_1,\\ldots,x_n$ which is the output of this random variable $X$ in course of $n$ independent trials.\n\nIn other words, we can say that we observe that i.i.d.r.v. $X_1,\\ldots, X_n$ with $X_i\\sim X$, $1\\leq i\\leq n$, take certain values: $X_1=x_1,\\ldots, X_n=x_n$.\n\nThe **likelihood**, or **likelihood function**, is the function $\\mathcal{L}(\\theta)=\\mathcal{L}(\\theta\\mid x_1,\\ldots,x_n)$ of the unknown parameter $\\theta$ (given the observed data $x_1,\\ldots,x_n$) which is equal to:\n\n- (if $X$ is a discrete random variable)\nthe probability to observe this data (given the value of the parameter $\\theta$):\n$$\n\\begin{aligned}\n\\mathcal{L}(\\theta)&=\\mathcal{L}(\\theta\\mid x_1,\\ldots,x_n)\\\\\n:&=\\mathbb{P}(X_1=x_1,\\ldots,X_n=x_n\\mid \\theta)\n\\\\&= \\mathbb{P}(X_1=x_1\\mid \\theta)\\ldots \\mathbb{P}(X_n=x_n\\mid \\theta).\n\\end{aligned}\n$$ {#eq-1.37}\n\n- (if $X$ is a continuous random variable with the PDF $f_X(x)=f_X(x\\mid\\theta)$)\n$$\n\\begin{aligned}\n\\mathcal{L}(\\theta)&=\\mathcal{L}(\\theta\\mid x_1,\\ldots,x_n)\\\\\n:&=f_X(x_1\\mid\\theta)f_X(x_2\\mid\\theta)\\ldots f_X(x_n\\mid\\theta).\n\\end{aligned}\n$$ {#eq-1.38}\n\n:::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-note icon=false}\n\n## `{r} sec`.`{r} cur` Maximum likelihood estimator\n\nThe **maximum likelihood estimator**$\\theta_*$ of the parameter $\\theta$ is the argument of the maximum of the likelihood function:\n$$\n\\theta_*=\\mathop{\\mathrm{argmax}}_\\theta\\mathcal{L}(\\theta),\n$$ {#eq-1.39}\nthat means that\n$$\n\\mathcal{L}(\\theta_*) = \\max_{\\theta}\\mathcal{L}(\\theta).\n$$ {#eq-1.40}\n\nThe standard approach to find $\\theta_*$ is to consider the **log-likelihood function**\n$$\nL(\\theta):=L(\\theta\\mid x_1,\\ldots,x_n)=\\ln \\mathcal{L}(\\theta\\mid x_1,\\ldots,x_n).\n$$ {#eq-1.41}\n\nThus, in the discrete case,\n$$\nL(\\theta) = \\ln \\mathbb{P}(X_1=x_1\\mid \\theta)+ \\ldots + \\ln \\mathbb{P}(X_n=x_n\\mid \\theta),\n$$ {#eq-1.42}\nwhereas, in the continuous case\n$$\nL(\\theta) = \\ln f_X(x_1\\mid\\theta)+ \\ldots + \\ln f_X(x_n\\mid\\theta),\n$$ {#eq-1.43}\n\nThen $\\theta_*$ is the point of maximum for both $\\mathcal{L}$ and $L$:\n$$\n\\theta_*=\\mathop{\\mathrm{argmax}}_\\theta L(\\theta)=\\mathop{\\mathrm{argmax}}_\\theta\\mathcal{L}(\\theta).\n$$ {#eq-1.44}\n:::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-warning icon=false}\n\n## `{r} sec`.`{r} cur` Example: Discrete case\n\nLet $X\\sim Po(\\lambda)$, i.e.\n$$\n\\mathbb{P}(X=k)=\\frac{\\lambda^k}{k!}e^{-\\lambda}, \\quad k\\geq0.\n$$\nSuppose we have a sample of $n$ values of $X$: $k_1,\\ldots,k_n$, and we want to estimate $\\lambda$ (here $\\theta=\\lambda$). Then\n$$\n\\begin{aligned}\n\\mathcal{L}(\\lambda)&=\\mathbb{P}(X=k_1\\mid\\lambda)\\ldots \\mathbb{P}(X=k_n\\mid\\lambda)\\\\\n& = \\frac{\\lambda^{k_1}}{k_1!}e^{-\\lambda}\\cdot\\ldots\\cdot \\frac{\\lambda^{k_n}}{k_n!}e^{-\\lambda}\\\\\n& = \\underbrace{\\frac{1}{k_1!\\ldots k_n!}}_{=: c>0}\\lambda^{k_1+\\ldots+k_n}e^{-\\lambda n},\n\\end{aligned}\n$$\nand therefore,\n$$\n\\begin{aligned}\nL(\\lambda)&=\\ln\\mathcal{L}(\\lambda)\n\\\\& = \\ln c + (k_1+\\ldots+k_n)\\ln\\lambda-\\lambda n.\n\\end{aligned}\n$$\nThen\n$$\nL'(\\lambda) = \\frac{k_1+\\ldots+k_n}{\\lambda}-n,\n$$\nand hence, $L'(\\lambda)=0$ iff\n$$\n{\\lambda = \\frac{k_1+\\ldots+k_n}{n}}.\n$$\n\nSince\n$$\nL''(\\lambda) = (L'(\\lambda))'=-\\frac{k_1+\\ldots+k_n}{\\lambda^2}<0,\n$$\nthe found value $\\lambda_* = \\frac{k_1+\\ldots+k_n}{n}$ is the point of maximum of $L$, hence, it is the maximum likelihood estimator for the parameter $\\lambda$.\n:::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-tip icon=false}\n\n## `{r} sec`.`{r} cur` Remark:  Relation to the LLN\n\nNote that, by the law of large numbers (LLN), if $X_i\\sim X\\sim Po(\\lambda)$, $i\\in\\mathbb{N}$, then\n$$\n\\frac{X_1+\\ldots+X_n}{n} \\to \\mathbb{E}(X)=\\lambda, \\qquad n\\to\\infty.\n$$\nIn other words, the maximum likelihood estimator converges to the theoretical value is the size of the sample converges to infinity.\n:::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-warning icon=false}\n\n## `{r} sec`.`{r} cur` Example: Continuous case\n\nLet $X\\sim Exp(\\lambda)$. Suppose we have a sample of $n$ values of $X$: $x_1,\\ldots,x_n$. Then\n$$\n\\begin{aligned}\n\\mathcal{L}(\\lambda) &= f_X(x_1\\mid \\lambda)\\ldots f_X(x_n\\mid \\lambda)\\\\\n&=\\lambda e^{-\\lambda x_1}\\ldots \\lambda e^{-\\lambda x_n}= \\lambda^n e^{-\\lambda(x_1+\\ldots +x_n)}.\n\\end{aligned}\n$$\n\nTherefore,\n$$\n\\begin{aligned}\nL(\\lambda)&= \\ln \\mathcal L(\\lambda)\n\\\\&= n \\ln \\lambda - \\lambda(x_1+\\ldots +x_n),\n\\end{aligned}\n$$\nso $L'(\\lambda)=0$ iff\n$$\n\\begin{gathered}\n\\frac{n}{\\lambda} -(x_1+\\ldots +x_n)=0,\\\\\n{\\lambda_*= \\frac{n}{x_1+\\ldots +x_n}}.\n\\end{gathered}\n$$\nNote that $L''(\\lambda)=-\\dfrac{n}{\\lambda^2}$, in particular, $L''(\\lambda_*)<0$, i.e. $\\lambda_*$ is indeed the maximum likelihood estimator for the parameter $\\lambda$.\n\nFinally, note that if $X_i\\sim X\\sim Exp(\\lambda)$, then, by LLN,\n$$\n\\frac{X_1+\\ldots+X_n}{n}\\to \\mathbb{E}(X)=\\frac{1}{\\lambda}, \\qquad n\\to\\infty.\n$$\n:::\n\n```{r}\n#| echo: false\ncur = cur + 1\n```\n\n::: {.callout-note icon=false}\n\n## `{r} sec`.`{r} cur` The method of moments\n\nIf the number of parameters is bigger than $1$, the maximum likelihood estimation may be challenging as one would need to find the point of maximum for a function of several variables (though it can be still effectively done numerically).\n\nAnother method to estimate unknown parameters of the distribution is the **method of moments**. Namely, assuming that we have a sample of $n$ values $x_1,\\ldots,x_n$ of a random variable $X$ which is believed to be followed a probability distribution which depends on $k$ unknown parameters $\\theta=(\\theta_1,\\ldots,\\theta_k)$.\n\nThen, we consider the first $k$ moments of the random variable $X$ (i.e. the moments of the population):\n$$\nm_j = \\mathbb{E}(X^j\\mid \\theta), \\qquad 1\\leq j\\leq k.\n$$\n\nNext, for the given data $x_1,\\ldots,x_n$, we consider $k$ averaged moments (i.e. the moments of the sample):\n$$\n\\overline{m}_j = \\frac{1}{n}\\sum_{l=1}^n x_l ^j, \\qquad 1\\leq j\\leq k.\n$$\n\nAfter this, we need to equate $k$ quantities:\n$$\nm_j = \\overline{m}_j, \\qquad 1\\leq j\\leq k,\n$$\nto determine $k$ unknown parameters $\\theta_1,\\ldots,\\theta_k$.\n:::\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":"auto","echo":false,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":false,"number-sections":true,"filters":["parse-latex"],"output-file":"Ch-01.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.549","mainfont":"Atkinson Hyperlegible","code-copy":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}