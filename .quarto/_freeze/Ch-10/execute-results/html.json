{
  "hash": "20574ed3b857ec996f74ed647e8aa834",
  "result": {
    "engine": "knitr",
    "markdown": "---\nformat: html\n---\n\n::: {.cell}\n\n:::\n\n\n# Time series\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note icon=false}\n\n## 10.1 Stochastic processes\n\n- Let $(\\Omega,\\mathcal{A},\\mathbb{P})$ be a probability space and $(S,\\mathcal{S})$ be a measurable **state space**.\n\n- Let $J$ be a set, whose points are usually interpreted as moments of time, i.e. $J\\subset\\mathbb{R}_+:=[0,\\infty)$.\n\n- A **stochastic process** (a.k.a. **random process**) is a collection of $S$-valued random variables *parametrized* by $t\\in J$:\n$$\n\\{X(t): \\Omega\\to S \\mid t\\in J\\}.\n$$\n\n- We have hence $X(t)=X(t,\\omega)$, $t\\in J$, $\\omega\\in\\Omega$, i.e. one can think about a stochastic process as a function of two variables.\n\n- We will normally omit $\\omega$ and **write $t$ as a subscript**:\n$$\nX_t:=X(t)=X(t,\\omega).\n$$\n\n-  If $J$ is continuous set, e.g. $J=\\mathbb{R}_+=[0,\\infty)$, then $\\{X_t\\mid t\\in J\\}$ is called a **continuous time** stochastic process.\n\n- If $J$ is a discrete set, e.g. $J=\\mathbb{Z}_+=\\{0,1,2,\\ldots\\}$, then $\\{X_t\\mid t\\in J\\}$ is called a **discrete time** stochastic process. \n\nIn this case, we will sometimes write $n$ instead of $t$.\n\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-caution icon=false}\n\n## 10.2 Definition: Sample path\n\nRecall that a stochastic process is a function of two variables, $t$ and $\\omega$. If we fix an $\\omega\\in\\Omega$, the set $\\{X_t(\\omega)=X(t,\\omega) \\mid t\\in J\\}$ is called a **sample path** of the process (for the chosen $\\omega$).\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-caution icon=false}\n\n## 10.3 Definition: Time series process\n\n**Time series** is the sample path of a discrete time stochastic process. \n\nOften, however, this term is used for the stochastic process itself. \n\nAnother term, which you may find in the literature, is **time series process**. \n\nWe will deal with real-valued time series, i.e. $3{S\\subset\\mathbb{R}}$. \n\nWe may also assume that $J\\subset\\mathbb{Z}$.\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-caution icon=false}\n\n## 10.4 Definition: Strictly stationary process\n\nA time series process $\\{X_t\\}$ is called \\term[stochastic process!stationary!strictly]{strictly stationary} if, for all $n\\in\\mathbb{N}$ and for all $k,t_1,\\ldots,t_n \\in J$ such that $t_1+k,\\ldots,t_n+k\\in J$, the joint distributions of random variables $X_{t_1},\\ldots, X_{t_n}$ and $X_{t_1+k},\\ldots, X_{t_n+k}$ are identical.\n\nIn other words, for all $\\Delta_1,\\ldots,\\Delta_n\\in\\mathcal{S}$,\n$$\n\\mathbb{P}\\bigl(  X_{t_1+k}\\in \\Delta_1, \\ldots, X_{t_n+k}\\in \\Delta_n\\bigr)\n$$ {#eq-strstat}\n**does not depend** on $k$.\n\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-tip icon=false}\n\n## 10.5 Remark\n\nStress that, normally, the (random) values of $X_t$ at different moments of time $t$ **are not independent**, hence, the probability in ([-@eq-strstat]) is **not** the product of the corresponding probabilities $\\mathbb{P}(  X_{t_i+k}\\in \\Delta_i)$.\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-caution icon=false}\n\n## 10.6 Definition: Weakly stationary process\n\nA time series process $\\{X_t\\}$ is called \\term[stochastic process!stationary!weakly]{weakly stationary}, if the following conditions hold:\n\n1) $\\mathbb{E}(X_t)$ does not depend on $t\\in J$ (i.e. it is a *constant* in $t$);\n\n2) $\\mathbb{E}(X_t^2)<\\infty$ for each $t\\in J$;\n\n3) For each $s\\in J$,\n$$\n\\begin{aligned}\n\\mathrm{cov}(X_t,X_{t+s}):&=\\mathbb{E}\\bigl((X_t-\\mathbb{E}(X_t))(X_s-\\mathbb{E}(X_s))\\bigr)\\notag\\\\\n& = \\mathbb{E}(X_t\\, X_{t+s})-\\mathbb{E}(X_t)\\,\\mathbb{E}(X_{t+s})\n\\end{aligned}\n$$\ndoes not depend on $t\\in J$ (assuming that $t+s\\in J$). \n\nEquivalently, $\\mathrm{cov}(X_t,X_s)$ depends on the **lag** $s-t$ only (for $s>t$).\n\nIn particular, $\\mathrm{Var}(X_t)=\\mathrm{cov}(X_t,X_t)$ does not depend on $t$ (here $s=0$).  \n\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-tip icon=false}\n\n## 10.7 Remark\n\nLet $\\{X_t\\}$ be a strictly stationary stochastic process and $\\mathbb{E}(X_t^2)<\\infty$. Then $\\{X_t\\}$ is weakly stationary.\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note icon=false}\n\n## 10.8 Autocovariance and autocorrelation functions\n\nFor a (weakly) stationary time series, \n\nthe function\n$$\n\\gamma_s:=\\mathrm{cov}(X_t,X_{t+s}), \\qquad s\\in J,\n$$\nis called the **autocovariance function**.\n\nFor a (weakly) stationary time series, the **autocorrelation function** is\n$$\n\\begin{aligned}\n\\rho_s:&=\\corr(X_t,X_{t+s})\\notag\\\\&=\\frac{\\mathrm{cov}(X_t,X_{t+s})}{\\sqrt{\\mathrm{Var}(X_t)}\\sqrt{\\mathrm{Var}(X_{t+s})}}=\\frac{\\gamma_s}{\\gamma_0}.\n\\end{aligned}\n$$\n\nLet $J=\\mathbb{Z}$. Then the autocovariance and autocorrelation functions are even functions.\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-warning icon=false}\n\n## 10.9 Example\n\nLet $Y$ and $Z$ be two *uncorrelated* identically distributed random variables with zero mean and variance $\\sigma^2$; i.e. $\\mathrm{cov}(Y,Z)=0$, $\\mathbb{E}(Y)=\\mathbb{E}(Z)=0$, $\\mathrm{Var}(Y)=\\mathrm{Var}(Z)=\\sigma^2$.  \n\nLet $\\lambda\\in[0,2\\pi]$ be a fixed number, and consider a continuous-time random process\n$$\nX_t = Y \\cos(\\lambda t) + Z\\sin (\\lambda t), \\quad t\\in\\mathbb{R}_+.\n$$ \nThen (check!)\n$$\nE(X_t)=0, \\quad E(X_t^2)=\\sigma^2<\\infty, \n$$\n\n$$\n\\begin{aligned}\n&\\quad \\mathrm{cov}(X_t,X_{t+s}) =\n\\mathbb{E}(X_tX_{t+s}) \\\\\n&= \\mathbb{E}\\bigl(Y^2\\cos(\\lambda t)\\cos(\\lambda (t+s))\n+Z^2\\sin(\\lambda t)\\sin(\\lambda (t+s))\\bigr)\n\\\\&=\\sigma^2 \\cos(\\lambda s)=:\\gamma_s,\n\\end{aligned}\n$$\nhence $\\{X_t\\}$ is weakly stationary, and $\\rho_s=\\cos(\\lambda s)$.\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-caution icon=false}\n\n## 10.10 Definition: White noise\n\nA time series process $\\{e_t\\}$ is called a **white noise** iff\n$$\n\\mathbb{E}(e_t)=0, \\qquad \\mathrm{cov}(e_t,e_{t+s})=\\begin{cases}\n\\sigma^2, & \\text{if } s=0,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nIn other words, the white noise is an example of a weakly stationary time series of uncorrelated random variables with zero mean.\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note icon=false}\n\n## 10.11 Backward shift and difference operators\n\nLet $X=\\{X_t\\}$ be a time series process.\n\n- The **backward shift operator** $B$ is given by\n$$\n(BX)_t = X_{t-1}.\n$$\n\n- The **difference operator** $\\nabla:=1\\!\\!1-B$  is given by\n$$\n(\\nabla X)_t = X_t-X_{t-1}.\n$$\n\n- Both operators can be applied repeatedly, e.g.\n$$\n\\begin{aligned}\n(B^2X)_t &= (BX)_{t-1}= X_{t-2},\\\\\n(\\nabla^2 X)_t & =X_t-2X_{t-1}+X_{t-2},\\\\\n(B\\nabla X)_t & =X_{t-1}-X_{t-2}=(\\nabla B X)_t.\n\\end{aligned}\n$$\n\n- This shows that $B$ and $\\nabla$ commute, that is clear also from $\\nabla=1\\!\\!1-B$.  \nThe latter formula allows to simplify calculations (henceforce, we will omit brackets: e.g. $\\nabla X_t$ instead of\n$(\\nabla X)_t$,  unless it may lead to misunderstanding):\n$$\n\\begin{aligned}\n\\nabla^3 X_t & = (1\\!\\!1-B)^3 X_t \\\\& = (1\\!\\!1-3B+3B^2-B^3)X_t \\\\\n& = X_t -3X_{t-1}+3X_{t-2}-X_{t-3}.\n\\end{aligned}\n$$\n\n-  We can also rewrite expressions using difference operators, e.g. \n$$\n\\begin{aligned}\n&\\quad X_t-5X_{t-1}+7X_{t-2}-3X_{t-3}\\\\\n&=X_t-X_{t-1}-4X_{t-1}+4X_{t-2}+3X_{t-2}-3X_{t-3}\\\\\n&=\\nabla X_t -4\\nabla X_{t-1}+3\\nabla X_{t-2}\\\\\n& =\\nabla X_t-\\nabla X_{t-1}-3\\nabla X_{t-1}+3\\nabla X_{t-2}\\\\\n& = \\nabla^2 X_t -3\\nabla^2 X_{t-1}.\n\\end{aligned}\n$$\n\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-warning icon=false}\n\n## 10.12 Example: Inversion relation\n\nLet $Y_t=\\nabla X_t=X_t-X_{t-1}$, $t\\in \\mathbb{N}$. Then\n$$\n\\begin{aligned}\nX_t&= Y_t+X_{t-1}= Y_t+Y_{t-1}+X_{t-2}=\\ldots\\\\\n&=X_0 +Y_t+Y_{t-1}+Y_{t-2}+\\ldots +Y_1\\\\\n& =X_0 +Y_t+BY_t+B^2Y_t + \\ldots +B^{t-1}Y_t\\\\\n& = X_0+(1\\!\\!1+B+B^2+\\ldots B^{t-1})Y_t,\n\\end{aligned}\n$$ \nwhere $1\\!\\!1 =B^0$ is the identity operator. \n\nNote that the last brackets contain $t$ summands.\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note icon=false}\n\n## 10.13 The autoregressive process, $AR(p)$\n\n- Let $p\\in\\mathbb{N}$. The **autoregressive process $AR(p)$** of the order $p$ is: \n$$\nX_t = \\mu + \\alpha_1 (X_{t-1}-\\mu)+\n\\ldots +\n\\alpha_p (X_{t-p}-\\mu) + e_t,\n$$ {#eq-arp}\nwhere $\\mu\\in\\mathbb{R}$ and $e_t$ is a white noise.\n\n- The relation ([-@eq-arp]) can be rewritten as follows:\n$$\n\\bigl( 1\\!\\!1 - \\alpha_1 B - \\ldots -\\alpha_p B^p\\bigr)(X_t-\\mu)=e_t.\n$$ {#eq-arp-bpow}\n\n- In particular, we will be interested to find the conditions when $X_t$ is weakly stationary.\n\n- Note that $AR(1)$ is the only autoregressive process which is a Markov chain. \n\nWe consider this case first, in more details.\n\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note icon=false}\n\n## 10.14 The first-order autoregressive process, $AR(1)$\n\nLet in ([-@eq-arp]) $p=1$ and $\\alpha:=\\alpha_1{{\\neq 0}}$. \n\nThen, iterating,\n$$\n\\begin{aligned}\nX_t-\\mu&=\\alpha(X_{t-1}-\\mu)+e_t\\\\\n&= \\alpha^t (X_0-\\mu)+\\sum_{k=0}^{t-1}\\alpha^ke_{t-k}.\n\\end{aligned} \n$$ {#eq-arp1}\nThen, since $\\mathbb{E}(e_t)=0$ for all $t$,  \n$$\n\\mathbb{E}(X_t)=\\mu+\\alpha^t (\\mathbb{E}(X_0)-\\mu).\n$$ \nTherefore, to have $\\mathbb{E}(X_t)$ independent on $t$, we would require ${\\mathbb{E}(X_0)=\\mu}$.  \n\nNext, **assuming that** $X_0$ and $e_t$ are independent for all $t$,  \nwe get, denoting $\\sigma^2:=\\mathrm{Var}(e_t)$,  \n$$\n\\begin{aligned}\n\\mathrm{Var}(X_t) &=\\alpha^{2t}\\mathrm{Var}(X_0)+\\sum_{k=0}^{t-1}\\alpha^{2k}\\sigma^2\\\\\n& = \\alpha^{2t}\\mathrm{Var}(X_0) + \\sigma^2 \\frac{1-\\alpha^{2t}}{1-\\alpha^2}\\\\\n& = \\frac{\\sigma^2}{1-\\alpha^2}+\\alpha^{2t}\\biggl(\\mathrm{Var}(X_0)-\\frac{\\sigma^2}{1-\\alpha^2}\\biggr),\n\\end{aligned}\n$$ \nprovided that $|\\alpha|\\neq 1$.  \nTherefore, to have $\\mathrm{Var}(X_t)$ independent on $t$, we would require ${\\mathrm{Var}(X_0)=\\frac{\\sigma^2}{1-\\alpha^2}}$ and $|\\alpha|\\neq1$. \n\n(Note that $|\\alpha|=1$ implies $\\mathrm{Var}(X_t)=\\mathrm{Var}(X_0)+\\sigma^2 t$.)\n\nHowever, the restrictions on $X_0$ are often not natural. \n\nMore importantly, on practice, people study processes which describe some established characteristics, which started to change a while ago, i.e., informally, the starting time was at \"$t=-\\infty$\".   \n\nMore rigorously, we assume $t\\in\\mathbb{Z}$ in ([-@eq-arp1]) and continue the iteration there (i.e. rewrite $X_0$ through $X_{-1}$ and so on). \n\nWe will get then:\n$$\nX_t-\\mu=\\sum_{k=0}^\\infty \\alpha^k e_{t-k}.\n$$ {#eq-wtf} \n\nTherefore, if ${|\\alpha|<1}$, then\n$$\n\\mathbb{E}(X_t)=\\mu, \\qquad \\mathrm{Var}(X_t) = \\frac{\\sigma^2}{1-\\alpha^2}.\n$$ \nMoreover, we have then\n$$\n\\begin{aligned}\n\\mathrm{cov}(X_t,X_{t+s})& = \\sum_{k=0}^\\infty\\sum_{j=0}^\\infty \\alpha^k \\alpha^j\\mathrm{cov}(e_{t-k} , e_{t+s-j})\\\\\n&= \\sum_{k=0}^\\infty\\alpha^k \\alpha^{k+s} \\sigma^2 =\\alpha^s \\frac{\\sigma^2}{1-\\alpha^2}.\n\\end{aligned}\n$$ \nHence, indeed, $X_t$ is a weakly stationary process with the autocovariance function $\\gamma_s=\\gamma_0 \\alpha^s$, \n\nand thus the autocorrelation function $\\rho_s=\\alpha^s$.\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note icon=false}\n\n## 10.15 Inverse operator to $1-\\alpha B$\n\nThe relation ([-@eq-arp1]) can be rewritten, see ([-@eq-arp-bpow]):\n$$\n(1\\!\\!1-\\alpha B)(X_t-\\mu)=e_t.\n$$ {#eq-Bform-ar1}\n\nNext, its solution ([-@eq-wtf]) can be also rewritten in terms of $B$:\n$$\nX_t-\\mu = \\sum_{k=0}^\\infty \\alpha^k B^ke_t.\n$$ \nIn other words, for ${|\\alpha|<1}$, there exists the inverse operator to $1-\\alpha B$:\n$$\n(1\\!\\!1-\\alpha B)^{-1}=\\sum_{k=0}^\\infty \\alpha^k B^k.\n$$\n\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note icon=false}\n\n## 10.16 The characteristic equation for $AR(1)$\n\nThe **characteristic equation** for $AR(1)$ ([-@eq-arp1]) is\n$$\n1-\\alpha z=0.\n$$ {#eq-cheqar1} \nTherefore (assuming that $\\alpha\\neq0$), ${|\\alpha|< 1}$ iff the only root of the characteristic equation ([-@eq-cheqar1]) $z=\\dfrac1{\\alpha}$ is such that ${|z|>1}$.\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note icon=false}\n\n## 10.17 The characteristic equation for $AR(p)$\n\nConsider now a general $p\\in\\mathbb{N}$ and $AR(p)$ process given by ([-@eq-arp]). \n\nSimilarly to $AR(1)$ case, we can, looking at its rewriting ([-@eq-arp-bpow]), construct the characteristic equation of $AR(p)$:\n$$ \n1\\!\\!1 - \\alpha_1 z -\\alpha_2 z^2 - \\ldots -\\alpha_p z^p=0.\n$$ {#eq-arp-cheq}\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-important icon=false}\n\n## 10.18 Theorem: Weakly stationary $AR(p)$ process\n\nLet all the roots of the characteristic equation ([-@eq-arp-cheq]) (including complex roots) lie outside the unit circle on the complex plain, \n\ni.e. let $|z_k|>1$ for all roots $z_1,\\ldots,z_p$ of ([-@eq-arp-cheq]). \n\nThen $AR(p)$ process ([-@eq-arp]) is weekly stationary. \n\nThe opposite is also true.\n\n:::\n\nExample: $AR(1)$, $\\alpha_1=\\alpha$, $z_1=\\frac{1}{\\alpha}$\n\n![Stationary vs. Nonstationary behaviour](images/statvsnonstat.png)\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note icon=false}\n\n## 10.19 The moving average process, $MA(q)$\n\n- Let $q\\in\\mathbb{N}$. The **moving average process, $MA(q)$** of the order $q$ is: \n$$\nX_t=\\mu+e_t+\\beta_1 e_{t-1}+\\ldots +\\beta_q e_{t-q},\n$$ {#eq-maq}\nwhere $\\mu\\in\\mathbb{R}$ and $e_t$ is the white noise.\n\n- It can be rewritten:\n$$\nX_t-\\mu =\\bigl( 1\\!\\!1+\\beta_1 B+\\beta_2 B^2 +\\ldots+\\beta_q B^q \\bigr)e_t.\n$$ {#eq-maqB}\n\n- Clearly, since $\\mathbb{E}(e_t)=$ and $\\mathrm{Var}(e_t)=\\sigma^2$, we have\n$$\n\\mathbb{E}(X_t)=\\mu, \\qquad \\mathrm{Var}(X_t)=\\sigma^2\\biggl(1+\\sum_{k=1}^q \\beta_k^2\\biggr).\n$$\n\n- Moreover, setting $\\beta_0:=1$, we have\n$$\n\\begin{aligned}\n\\mathrm{cov}(X_t,X_{t+s})&=\\mathbb{E}(X_t\\, X_{t+s})\\\\&= \\sum_{k=0}^q\\sum_{j=0}^q \\beta_k\\beta_j \\mathrm{cov}(e_{t-k},e_{t+s-j})\\\\\n& = \\sum_{k=0}^{q-s}\\beta_k\\beta_{k+s} \\sigma^2.\n\\end{aligned}\n$$ \nIn particular, any $MA(q)$ process is weakly stationary.\n\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-caution icon=false}\n\n## 10.20 Definition: Invertible $MA(q)$ process\n\nAn $MA(q)$ process ([-@eq-maq]) is called **invertible**, \n\nif the noise at time $t$ can be represented through the values of the process at times $s\\leq t$:   \n$$ \ne_t =\\mathrm{const}+ X_t  +\\gamma_1 X_{t-1}+\\ldots+\\gamma_q X_{t-q}+\\ldots,\n$$ {#eq-invprop}\nfor some $\\gamma_k\\in\\mathbb{R}$, such that {$\\sum\\limits_{k=1}^\\infty\\gamma_k^2<\\infty$}.\n \n\nIn other words $MA(q)$ process is invertible if the operator $1\\!\\!1+\\beta_1 B +\\ldots+\\beta_q B^q $ in ([-@eq-maqB]) is invertible.\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-important icon=false}\n\n## 10.21 Theorem: Invertible $MA(q)$ process\n\nLet $MA(q)$ process ([-@eq-maq]) be given. Consider its **characteristic equation**\n$$ \n1+\\beta_1z+\\beta_2 z^2+\\ldots+\\beta_q z^q=0.\n$$ {#eq-cheqmaq}\n\nLet all the roots of this characteristic equation (including complex roots) lie outside the unit circle on the complex plain, i.e. let $|z_k|>1$ for all roots $z_1,\\ldots,z_q$ of ([-@eq-cheqmaq]). \n\nThen $MA(q)$ process ([-@eq-maq]) is invertible. \n\nThe opposite statement is also true.\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-tip icon=false}\n\n## 10.22 Remark\n\nNote that $AR(p)$ process is always invertible in this sense, by the very definition ([-@eq-arp]).\n\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note icon=false}\n\n## 10.23 Invertible $MA(1)$ process\n\nFor $q=1$, the proof of the previous theorem becomes especially simple. \n\nNamely, then (denoting $\\beta:=\\beta_1$) \n$$\n\\begin{aligned}\ne_t&=X_t-\\mu - \\beta e_{t-1}\\\\&=X_t-\\mu - \\beta (X_{t-1}-\\mu - \\beta e_{t-2})=\\ldots\n\\\\&= (X_t-\\mu) -\\beta (X_{t-1}-\\mu)\\\\&\\quad +\\beta^2 (X_{t-2}-\\mu)-\\beta^3(X_{t-3}-\\mu)+\\ldots\n\\\\&=c+\\sum_{k=0}^\\infty (-\\beta)^k X_{t-k},\n\\end{aligned}\n$$ \nwhere, {for $|\\beta|<1$ only},\n$$\nc= -\\mu\\sum_{k=0}^\\infty (-\\beta)^k=-\\frac{\\mu}{1+\\beta}.\n$$ \nLet $\\gamma_k:=(-\\beta)^k$ with $|\\beta|<1$, then we get  \n$$\n\\sum\\limits_{k=1}^\\infty \\gamma_k^2=\\frac{1}{1-\\beta^2}<\\infty.\n$$  \nActually, we have shown that, for $|\\beta|<1$,\n$$\n(1\\!\\!1+\\beta B)^{-1} = 1\\!\\!1-\\beta B+\\beta^2 B^2-\\beta^3 B^3+\\ldots.\n$$ \nThe characteristic equation is then $1+\\beta z=0$ and its the only root is $z=-\\frac{1}{\\beta}$, thus, $|z|>1$ iff $|\\beta|<1$.\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note icon=false}\n\n## 10.24 The autoregressive moving average process, $ARMA(p,q)$\n\nWe consider now a natural combination of $AR(p)$ and $MA(q)$. \n\nLet\n$$\n\\begin{aligned}\nX_t-\\mu& = \\alpha_1 (X_{t-1}-\\mu)+\\ldots +\\alpha_p (X_{t-p}-\\mu) \\notag\n\\\\&\\quad+ e_t+\\beta_1 e_{t-1}+\\ldots +\\beta_q e_{t-q}.\n\\end{aligned}\n$$ \nOr, it can be rewritten:\n$$\n\\begin{multlined}\n(1\\!\\!1-\\alpha_1 B - \\ldots -\\alpha_p B^p)(X_t-\\mu)\\\\=\n(1\\!\\!1+\\beta_1 B+\\ldots+\\beta_q B^q)e_t.\n\\end{multlined}\n$$ {#eq-multl45}\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-important icon=false}\n\n## 10.25 Theorem \n\nAn $ARMA(p,q)$ process if weakly stationary iff its $AR(p)$ is weakly stationary, i.e. iff all the roots of\n$$\n1 - \\alpha_1 z -\\alpha_2 z^2 - \\ldots -\\alpha_p z^p=0\n$$\nlie outside the unit circle on the complex plane.\n\n \nAn $ARMA(p,q)$ process is invertible (i.e. there exists an expansion ([-@eq-invprop])) iff its $MA(q)$ is invertible, i.e. iff all the roots of\n$$\n1+\\beta_1z+\\beta_2 z^2+\\ldots+\\beta_q z^q=0\n$$\nlie outside the unit circle on the complex plane.\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-tip icon=false}\n\n## 10.26 Remark\n\nThe expansion ([-@eq-wtf]) shows that, actually, any $AR(1)$ process with $|\\alpha|<1$ is an $MA(\\infty)$ process with $\\beta_k=\\alpha^k$, the latter means, by definition, an {infinite} sum in ([-@eq-maq]) with $\\sum\\limits_{k}\\beta_k^2<\\infty$.\n \nMoreover, it can be shown that any $AR(p)$ process is an $MA(\\infty)$ process in this sense.\n\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note icon=false}\n\n## 10.27 Non-stationary process: $ARIMA(p,j,q)$\n\nA process $X_t$ is called an $ARIMA(p,j,q)$ process if $X_t$ is not weakly stationary, but $\\nabla^j X_t$ is a weakly stationary $ARMA(p,q)$ process.\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-warning icon=false}\n\n## 10.28 Example \n\nLet $X_t=X_{t-1}+e_t$, then $X_t$ is not weakly stationary, \nbut $Y_t=\\nabla X_t = e_t$ is weakly stationary $ARMA(0,0)$ process, \nthus $X_t$ is an $ARIMA(0,1,0)$ process.\n::: \n\nDenote $Y_t=\\nabla^j X_t=(1\\!\\!1-B)^j X_t$. \n\nAssume, for simplicity, that $\\mathbb{E}(X_t)=0$ (but $\\mathrm{Var}(X_t)$ depends on $t$), then $\\mathbb{E}(Y_t)=0$ and if $Y_t$ is an $ARMA(p,q)$, it should satisfy ([-@eq-multl45]) with $\\mu=0$.\n\nThen ([-@eq-multl45]) takes the form\n$$\n\\begin{multlined}\n(1\\!\\!1-\\alpha_1B-\\ldots-\\alpha_p B^p)(1\\!\\!1-B)^j X_t \\\\=(1\\!\\!1+\\beta_1 B+\\ldots +\\beta_q B^q)e_t.\n\\end{multlined} \n$$\n\nTherefore, if we consider the $AR$-characteristic equation for process $X_t$, it will have the form\n$$\n(1 - \\alpha_1 z -\\alpha_2 z^2 - \\ldots -\\alpha_p z^p)(1-z)^j=0,\n$$\ni.e. it has the root $1$ of multiplicity $j$. If all other roots  lie outside the unit circle, $X_t$ is an $ARIMA(p,j,q)$ process.\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-warning icon=false}\n\n## 10.29 Example\n\nLet\n$$\nX_t=0.6X_{t-1}+0.3 X_{t-2}+0.1X_{t-3}+e_t-e_{t-1}.\n$$ \nThe $AR$-characteristic equation is\n$$\n1-0.6z-0.3z^2-0.1z^3=0.\n$$ \nIt is easy to see that $z=1$ is a root. \n\nUsing the long division rule, or just rewriting\n$$\n\\begin{gathered}\n1-z+ 0.4z-0.4z^2+0.1z^2-0.1z^3=0, \\\\\n(1-z)(1+0.4z+0.1z^2)=0,\n\\end{gathered}\n$$\nwe get that\n$$\nz^2+4z+10=0, \\qquad z=-2\\pm i\\sqrt{6},\n$$\nand $|z|=\\sqrt{10}>1$.\n\nTherefore, $X_t$ is an $ARIMA(2,1,1)$ process, \n\ni.e. $\\nabla X_t$ is a weakly stationary $ARMA(2,1)$ process.\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}