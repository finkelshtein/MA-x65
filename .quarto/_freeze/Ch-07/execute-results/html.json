{
  "hash": "c3c95f97aa2cfccd70ab3d8e59649ba5",
  "result": {
    "engine": "knitr",
    "markdown": "---\nformat: html\n---\n\n::: {.cell}\n\n:::\n\n\n# Estimations of lifetime distributions\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note icon=false}\n\n## 7.1 Why do we need to estimate\n\n- The main problem about the force of mortality $\\mu_t$ (a.k.a. the hazard rate $\\lambda_t$) is that it is unknown in practice.\n\n- Instead, one has data that is the result of some observations. Observations are made at discrete moments of time.\n\n- We consider the case when the age $x$ with which persons are entering into the observation is either irrelevant or uniform (i.e. the whole observed population has \"almost\" the same age).\n\n- Hence, time starts at $0$ with the start of observation and all changes are recorded at discrete moments of time only.\n\n- Let $t_1<t_2<t_3<\\ldots$ be ordered moments of time when the records where updated (e.g. days when deaths happened). Stress that the number of events at each $t_j$ may be bigger than $1$.\n\n- The moments of time are random. Our basic \\textit{ad hoc} assumption is that all $t_j$ have independent identical distribution.\n\n- We assume hence that $T$ changes at times $t_1, t_2,\\ldots$ only, hence, $F(t)=\\mathbb{P}(T\\leq t)$ is a step-function: it has jumps at $t_j$, $j\\geq1$, and it takes constant value in between.\n\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note icon=false}\n\n## 7.2 Discrete hazard function\n\nWe define\n$$\n\\lambda_j = \\mathbb{P}(T=t_j \\mid T\\geq t_j), \\quad j\\geq1.\n$$\n\nThen\n$$\n\\lambda_j=\\frac{\\mathbb{P}(T=t_j)}{\\mathbb{P}(T\\geq t_j)}=\n\\frac{\\mathbb{P}(T=t_j)}{\\sum\\limits_{k\\geq j}\\mathbb{P}(T=t_k)}.\n$$\n\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note icon=false}\n\n## 7.3 Estimator for the discrete hazard function\n\n- Let $d_j$ be the number of deaths at time $t_j$, and $n_j$ be the number of persons in the risk set prior to the time $t_j$.\n\n- Stress that all persons **censored** (i.e. excluded from the observation) before time $t_j$ are not counted in $n_j$.\n\n- We assume also that any person censored at time $t_j$ is actually censored immediately after that time, and hence is counted in $n_j$.\n\n- The estimator for $\\lambda_j$ is then\n$$\n\\hat{\\lambda}_j=\\frac{d_j}{n_j}.\n$$ {#eq-est_for_la_j}\n\n- We will see below why this estimator is relevant.\n\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note icon=false}\n\n## 7.4 Discrete survival function\n\n- Since $S(t)=1-F(t)$, the survival function $S(t)$ is also constant on each $[t_j,t_{j+1})$, $j\\geq 1$: namely, for $t\\in [t_j,t_{j+1})$,\n$$\n\\begin{aligned}\nS(t)&=\\mathbb{P}(T>t)=\\mathbb{P}(T>t_j)\\\\\n& = \\mathbb{P}(T>t_j\\mid T>t_{j-1})\\mathbb{P}(T>t_{j-1})\\\\\n& = \\mathbb{P}(T>t_j\\mid T>t_{j-1})\\\\\n&\\qquad\\times \\mathbb{P}(T>t_{j-1}\\mid T>t_{j-2})\\mathbb{P}(T>t_{j-2})\\\\\n& = (1-\\lambda_j)(1-\\lambda_{j-1})\\ldots\n(1-\\lambda_{2})\\mathbb{P}(T>t_{1})\\\\\n&= \\prod_{i=1}^j (1-\\lambda_i),\n\\end{aligned}\n$$ {#eq-survdiscr}\nwhere we used that $t_1$ is the first recorded moment of time, i.e. $T\\geq t_1$, hence,\n$$\n\\mathbb{P}(T>t_{1})=1-\\mathbb{P}(T=t_{1})=1-\\lambda_1.\n$$\n\n- Therefore, the value of $S(t)$ on the interval $t\\in[t_{j},t_{j+1})$ is the value on the previous interval $[t_{j-1},t_j)$ multiplied by $1-\\lambda_j$.\n\n- One can aso rewrite ([-@eq-survdiscr]) as follows:\n$$\nS(t)=\\prod_{j:t_j\\leq t} (1-\\lambda_j).\n$$ {#eq-survdiscr1}\n\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note icon=false}\n\n## 7.5 Kaplan--Meier estimate of the survival function\n\n- **Kaplan--Meier estimate** $\\hat{S}(t)$ of the survival function $S(t)$ is just the replacing of $\\lambda_i$ in ([-@eq-survdiscr1]) by $\\hat{\\lambda}_j$ given by ([-@eq-est_for_la_j]):\n$$\n\\hat{S}(t)=\\prod_{j:t_j\\leq t} (1-\\hat{\\lambda_j})=\\prod_{j:t_j\\leq t} \\Bigl(1-\\frac{d_j}{n_j}\\Bigr).\n$$ {#eq-KMest}\n\n- We are going to discuss now in which sense $\\hat{\\lambda}_j$ and $\\hat{S}(t)$ are relevant estimators.\n\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-caution icon=false}\n\n## 7.6 Definition: Likelihood\n\nLet $X:\\Omega\\to S$ be a discrete random variable dependent on a parameter. Consider the probability that $X=x\\in S$ given the specific value $\\theta$ of the parameter. It can be denoted $\\mathbb{P}_\\theta(X=x)$, or $\\mathbb{P}(X=x\\mid \\theta)$ (despite $\\theta$ does not need to be random).\n\nThe **likelihood function** is the function $\\mathcal{L}(\\theta\\mid x)$ of $\\theta$ which depends on $x$, and it is given by\n$$\n\\mathcal{L}(\\theta\\mid x)=\\mathbb{P}(X=x\\mid \\theta).\n$$\nWe say that $\\mathcal{L}$ is the likelihood function, **given the outcome** $x$ of the random variable $X$.\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-tip icon=false}\n\n## 7.7 Remark:  Likelihood is not probability\n\nStress that $\\mathcal{L}(\\theta \\mid x)$ is not $\\mathbb{P}(\\theta \\mid X=x)$. First, the latter object does not have sense if $\\theta$ is not random. However, even for a random variable $\\theta$,\n$$\n\\begin{aligned}\n\\mathbb{P}(\\theta=\\theta_0\\mid X=x)&=\n\\frac{\\mathbb{P}( X=x\\mid \\theta=\\theta_0) \\mathbb{P}(\\theta=\\theta_0)}{\\mathbb{P}(X=x)}\\\\&= \\mathcal{L}(\\theta_0)\\frac{\\mathbb{P}(\\theta=\\theta_0)}{\\mathbb{P}(X=x)},\n\\end{aligned}\n$$\naccording to Bayes' theorem.\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note icon=false}\n\n## 7.8 Likelihood function of the data\n\n- Fix $j\\geq 1$, and consider the random variable $X$ that is the vector of all $d_i$ and $n_i$ for $1\\leq i\\leq j$.\n\n- Let $\\theta$ be the vector of all $\\lambda_i$, $1\\leq i\\leq j$. Then\n$$\n\\begin{aligned}\n\\mathcal{L}(\\theta\\mid X)&=\\mathbb{P}\\bigl(X=(d_i,n_i:i\\leq j)\\bigm\\vert \\theta=(\\lambda_i:i\\leq j)\\bigr)\n\\\\&= \\prod_{i=1}^j \\binom{n_i}{d_i}\\lambda_i^{d_i}(1-\\lambda_i)^{n_i-d_i}.\n\\end{aligned}\n$$\n\n- A natural question is to find the **maximal likelihood**:\n$$\n\\theta_*=\\mathrm{argmax}\\, \\mathcal{L}(\\theta\\mid X),\n$$\ni.e. $\\mathcal{L}(\\theta_*\\mid X)=\\max\\limits_{\\theta} \\mathcal{L}(\\theta\\mid X)$.\n\n\n\n- The maximal likelihood is **the value of the parameter that maximizes the probability to observe the data**.\n\n- Since logarithm is an increasing function,\n$$\n\\mathrm{argmax}\\, \\mathcal{L}(\\theta\\mid X)=\\mathrm{argmax}\\, \\ln \\mathcal{L}(\\theta\\mid X).\n$$\n\n- We have\n$$\n\\begin{aligned}\n\\ln \\mathcal{L}(\\theta\\mid X)&=\n\\sum_{i=1}^j \\bigl(d_i\\ln \\lambda_i + (n_i-d_i)\\ln (1-\\lambda_i)\\bigr)\\\\\n&\\quad +\\sum_{i=1}^j\\ln\\binom{n_i}{d_i}.\n\\end{aligned}\n$$\n\n- The last summand is a constant in $\\theta$ (for the given data $X$), hence it will not influence the maximal likelihood. One has\n$$\n\\begin{aligned}\n\\frac{\\partial \\ln \\mathcal{L}(\\theta\\mid X)}{\\partial \\lambda_i} =\\frac{d_i}{\\lambda_i} - \\frac{n_i-d_i}{1-\\lambda_i}.\n\\end{aligned}\n$$\n\n- The derivative is $0$ for $\\lambda_i= \\dfrac{d_i}{n_i}$.\n\n- Moreover, it can be checked that\n$$\n\\theta = \\Bigl(\\dfrac{d_i}{n_i}\\Bigr)_{i\\leq j}=(\\hat{\\lambda}_i)_{i\\leq j}\n$$\nis indeed the point of maximum of $\\ln \\mathcal{L}(\\theta\\mid X)$.\n\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note icon=false}\n\n## 7.9 Nelson--Aalen estimate of the integrated hazard\n\n- Let $\\mu_t$ be the hazard function (a.k.a. the force of mortality). Then the function\n$$\n\\Lambda_t:=\\int_0^t\\mu_s\\,ds\n$$\nis called the **integrated hazard** function.\n\n- Recall that\n$$\nS_x(t)={}_{t}p_{x}=\\exp\\biggl(-\\int_0^t\\mu_{x+s}\\,ds\\biggr),\n$$\nand $S(t)=S_0(t)=\\mathbb{P}(T>t)$ (where $0$, recall, does not need to be an absolute age, but rather an initial time of the observation). Therefore,\n$$\nS(t)=\\exp(-\\Lambda_t).\n$$\n\n- The **Nelson--Aalen estimate** of the integrated hazard is\n$$\n\\hat{\\Lambda}_t = \\sum_{j: t_j\\leq t} \\dfrac{d_j}{n_j};\n$$\nand the corresponding estimate of the survival function is\n$$\n\\hat{S}(t)=\\exp\\bigl(-\\hat{\\Lambda_t}\\bigr).\n$$\n\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-note icon=false}\n\n## 7.10 Likelihood function for continuous $T$\n\nFor a continuous random variable $T$, the likelihood function is usually defined through their densities $f(t\\mid\\theta)$ rather than probabilities $F(T\\mid\\theta)$. In particular, if deaths happened at times $t_1,t_2,\\ldots$ then (assuming that all lives had independent identical distributions):\n$$\n\\mathcal{L}(\\theta|\\textrm{data}):=\n\\prod_{t_j} f(t_j\\mid \\theta).\n$$\nIf the time of death is a discrete random variable, its density is just the probability, i.e. the formula is agreed with the previous one.\n\nIf, however, a live was censored at a time $t_k$, it means that it was still alive at that time, so we have to multiply the likelihood by the probability $S(t_k\\mid \\theta)$.\n\nTherefore:\n$$\n\\mathcal{L}(\\theta|\\textrm{data}):=\n\\prod_{\\textrm{times of deaths}} f(t_j\\mid \\theta)\\prod_{\\textrm{times of censoring}} S(t_j\\mid \\theta).\n$$\nSince, by (1.11), $f(t)=S(t)\\lambda(t)$, where $\\lambda(t)=\\mu_t$ is the hazard rate, we can also rewrite:\n$$\n\\mathcal{L}(\\theta|\\textrm{data}):=\n\\prod_{\\textrm{times of deaths}} \\lambda(t_j\\mid \\theta)\\prod_{\\textrm{all times}} S(t_j\\mid \\theta).\n$$\n\nStress that some $t_j$ **may be equal**.\n\nA common use of such likelihood is for the estimation of parameters for the parametric models.\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-warning icon=false}\n\n## 7.11 Example\n\nSuppose we expect that certain data corresponds to the constant force of mortality $\\mu_t\\equiv \\mu$ (or $\\lambda(t)\\equiv\\mu$ in another notations). Then $S(t)=\\exp(-\\mu t)$, and we have (here $\\theta=\\mu$)\n$$\n\\begin{aligned}\n\\mathcal{L}(\\mu) &= \\prod_{\\textrm{times of deaths}} \\mu\\prod_{\\textrm{all times}} \\exp(-\\mu t) \\\\\n& = \\mu^k \\exp\\Bigl(-\\mu \\sum_{j}t_j\\Bigr),\n\\end{aligned}\n$$\nwhere $k$ is the total number of the observed deaths and $\\{t_j\\}$ are times of **all events** (both deaths and censoring). Then\n$$\nl(\\mu) = \\ln \\mathcal{L}(\\mu) = k\\ln \\mu - \\mu \\sum_{j}t_j,\n$$\nand hence,\n$$\nl'(\\mu) =\\frac{k}{\\mu} - \\sum_{j}t_j, \\quad L''(\\mu) = -\\frac{k}{\\mu^2}<0.\n$$\nThus,\n$$\n\\mu_* = \\frac{k}{\\displaystyle\\ \\sum_{j}t_j\\ }\n$$\nis the estimator for $\\mu$ which maximize the likelihood.\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}